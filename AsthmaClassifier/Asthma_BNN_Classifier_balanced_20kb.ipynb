{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asthma BNN Classifier (Balanced, ≤ 20KB params)\n",
    "\n",
    "This notebook trains a compact **Binarized Neural Network (BNN)** to predict asthma.\n",
    "\n",
    "**What this version does**\n",
    "- Drops **`Ethnicity`** from predictors\n",
    "- Uses **unweighted BCE** (no `pos_weight`)\n",
    "- **Oversamples positives** on the train split (avoid collapse without changing batch sizes)\n",
    "- **Hidden sizes = [64, 32]** to keep total params well **below 20KB** (ESP32-friendly)\n",
    "- Inference **matches training**: `fc → BN → hardtanh → dropout` (dropout disabled at eval)\n",
    "- Saves artifacts to **`/mnt/data/bnn_artifacts`** (unchanged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4536, 29)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Setup ===\n",
    "import os, json, math, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "CSV_PATH = Path('asthma_disease_data.csv')  # (unchanged path)\n",
    "assert CSV_PATH.exists(), f'CSV not found: {CSV_PATH}'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Clean & Feature Selection (drop `Ethnicity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns (examples): ['DoctorInCharge', 'EducationLevel', 'Ethnicity', 'LungFunctionFEV1', 'LungFunctionFVC', 'PatientID'] ...\n",
      "Predictors (22): ['Age', 'Gender', 'BMI', 'Smoking', 'PhysicalActivity', 'DietQuality', 'SleepQuality', 'PollutionExposure', 'PollenExposure', 'DustExposure', 'PetAllergy', 'FamilyHistoryAsthma', 'HistoryOfAllergies', 'Eczema', 'HayFever', 'GastroesophagealReflux', 'Wheezing', 'ShortnessOfBreath', 'ChestTightness', 'Coughing', 'NighttimeSymptoms', 'ExerciseInduced']\n",
      "Target: Diagnosis\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>DietQuality</th>\n",
       "      <th>SleepQuality</th>\n",
       "      <th>PollutionExposure</th>\n",
       "      <th>PollenExposure</th>\n",
       "      <th>DustExposure</th>\n",
       "      <th>...</th>\n",
       "      <th>Eczema</th>\n",
       "      <th>HayFever</th>\n",
       "      <th>GastroesophagealReflux</th>\n",
       "      <th>Wheezing</th>\n",
       "      <th>ShortnessOfBreath</th>\n",
       "      <th>ChestTightness</th>\n",
       "      <th>Coughing</th>\n",
       "      <th>NighttimeSymptoms</th>\n",
       "      <th>ExerciseInduced</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>15.848744</td>\n",
       "      <td>0</td>\n",
       "      <td>0.894448</td>\n",
       "      <td>5.488696</td>\n",
       "      <td>8.701003</td>\n",
       "      <td>7.388481</td>\n",
       "      <td>2.855578</td>\n",
       "      <td>0.974339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>22.757042</td>\n",
       "      <td>0</td>\n",
       "      <td>5.897329</td>\n",
       "      <td>6.341014</td>\n",
       "      <td>5.153966</td>\n",
       "      <td>1.969838</td>\n",
       "      <td>7.457665</td>\n",
       "      <td>6.584631</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>18.395396</td>\n",
       "      <td>0</td>\n",
       "      <td>6.739367</td>\n",
       "      <td>9.196237</td>\n",
       "      <td>6.840647</td>\n",
       "      <td>1.460593</td>\n",
       "      <td>1.448189</td>\n",
       "      <td>5.445799</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender        BMI  Smoking  PhysicalActivity  DietQuality  \\\n",
       "0   63       0  15.848744        0          0.894448     5.488696   \n",
       "1   26       1  22.757042        0          5.897329     6.341014   \n",
       "2   57       0  18.395396        0          6.739367     9.196237   \n",
       "\n",
       "   SleepQuality  PollutionExposure  PollenExposure  DustExposure  ...  Eczema  \\\n",
       "0      8.701003           7.388481        2.855578      0.974339  ...       0   \n",
       "1      5.153966           1.969838        7.457665      6.584631  ...       0   \n",
       "2      6.840647           1.460593        1.448189      5.445799  ...       0   \n",
       "\n",
       "   HayFever  GastroesophagealReflux  Wheezing  ShortnessOfBreath  \\\n",
       "0         0                       0         0                  0   \n",
       "1         0                       0         1                  0   \n",
       "2         1                       0         1                  1   \n",
       "\n",
       "   ChestTightness  Coughing  NighttimeSymptoms  ExerciseInduced  Diagnosis  \n",
       "0               1         0                  0                1          0  \n",
       "1               0         1                  1                1          0  \n",
       "2               1         0                  1                1          0  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_CANDIDATES = ['Diagnosis','asthma','Asthma','is_asthmatic']\n",
    "target_col = next((c for c in TARGET_CANDIDATES if c in df.columns), None)\n",
    "assert target_col is not None, f'Target column not found among {TARGET_CANDIDATES}'\n",
    "\n",
    "# Drop obvious identifiers and high-cardinality free text\n",
    "drop_cols = set()\n",
    "for c in df.columns:\n",
    "    if c == target_col:\n",
    "        continue\n",
    "    if c.lower() in ['patientid','doctorincharge','id','uuid','guid','recordid','ethnicity','educationlevel','lungfunctionfev1', 'lungfunctionfvc',]:\n",
    "        drop_cols.add(c)\n",
    "    if df[c].dtype == 'object' and df[c].nunique(dropna=True) > 20:\n",
    "        drop_cols.add(c)\n",
    "\n",
    "# Remove all-null, near-constant, duplicates\n",
    "for c in df.columns:\n",
    "    if c == target_col or c in drop_cols:\n",
    "        continue\n",
    "    s = df[c]\n",
    "    if s.isna().all():\n",
    "        drop_cols.add(c)\n",
    "    else:\n",
    "        top_freq = s.value_counts(dropna=False).iloc[0] / len(s)\n",
    "        if top_freq > 0.95:\n",
    "            drop_cols.add(c)\n",
    "\n",
    "seen = {}\n",
    "for c in df.columns:\n",
    "    if c == target_col or c in drop_cols:\n",
    "        continue\n",
    "    key = tuple(pd.util.hash_pandas_object(df[c].fillna('__NA__')).values)\n",
    "    if key in seen:\n",
    "        drop_cols.add(c)\n",
    "    else:\n",
    "        seen[key] = c\n",
    "\n",
    "df_clean = df.drop(columns=list(drop_cols), errors='ignore').copy()\n",
    "\n",
    "# Keep only numeric predictors\n",
    "predictors = [c for c in df_clean.columns if c != target_col and pd.api.types.is_numeric_dtype(df_clean[c])]\n",
    "df_clean = df_clean[predictors + [target_col]]\n",
    "\n",
    "# --- Enforce dropping Ethnicity ---\n",
    "if 'Ethnicity' in df_clean.columns:\n",
    "    df_clean = df_clean.drop(columns=['Ethnicity'])\n",
    "predictors = [c for c in df_clean.columns if c != target_col]\n",
    "\n",
    "print('Dropped columns (examples):', sorted(list(drop_cols))[:8], '...')\n",
    "print('Predictors ({}):'.format(len(predictors)), predictors)\n",
    "print('Target:', target_col)\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Split & Scale (unchanged batch sizes later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test: (3628, 22) (908, 22)\n",
      "Base positive rate (train): 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df_clean[target_col].astype(int).values\n",
    "assert set(np.unique(y)).issubset({0,1}), 'Target must be binary 0/1.'\n",
    "X = df_clean[predictors].astype(np.float32).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "x_mean = scaler.mean_.astype(np.float32)\n",
    "x_scale= scaler.scale_.astype(np.float32)\n",
    "\n",
    "print('Train/Test:', X_train.shape, X_test.shape)\n",
    "print('Base positive rate (train):', float((y_train==1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Oversample Positives (no pos_weight; no path/batch-size changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {0: 1814, 1: 1814}\n",
      "After : {0: 1814, 1: 1814}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_bal, y_train_bal = X_train, y_train\n",
    "print('Before:', dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print('After :', dict(zip(*np.unique(y_train_bal, return_counts=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) BNN Model (hidden=[64,32], param ≤ 20KB, exact forward at train & infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d52fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# ---------- binarizers ----------\n",
    "class SignBinarizeZeroToPlusOne(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x): return torch.where(x >= 0, torch.ones_like(x), -torch.ones_like(x))\n",
    "    @staticmethod\n",
    "    def backward(ctx, g): return g.clamp_(-1, 1)\n",
    "\n",
    "class SignBinarizeLegacy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x): return x.sign()   # -1, 0, +1 (0 stays 0)\n",
    "    @staticmethod\n",
    "    def backward(ctx, g): return g.clamp_(-1, 1)\n",
    "\n",
    "def make_binarize(mode: str):\n",
    "    mode = str(mode).lower()\n",
    "    assert mode in {\"legacy\",\"z2p1\"}\n",
    "    return SignBinarizeLegacy.apply if mode==\"legacy\" else SignBinarizeZeroToPlusOne.apply\n",
    "\n",
    "# ---------- model ----------\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=False, binarize_fn=None):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        nn.init.kaiming_normal_(self.weight, nonlinearity='relu')\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "        self.binarize = binarize_fn or make_binarize(\"legacy\")\n",
    "    def forward(self, x):\n",
    "        x_b = self.binarize(x)\n",
    "        alpha = self.weight.detach().abs().mean(dim=1, keepdim=True)\n",
    "        w_b = self.binarize(self.weight) * alpha\n",
    "        return F.linear(x_b, w_b, self.bias)\n",
    "\n",
    "class HybridBNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=[32,64,128, 128, 64, 64, 32], p_drop=0.1, binarize_mode=\"legacy\"):\n",
    "        super().__init__()\n",
    "        self.binarize = make_binarize(binarize_mode)\n",
    "        self.float1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden[0], bias=False),\n",
    "            nn.BatchNorm1d(hidden[0]),\n",
    "            nn.Hardtanh()\n",
    "        )\n",
    "        blocks = []\n",
    "        for i in range(len(hidden)-1):\n",
    "            blocks += [\n",
    "                BinaryLinear(hidden[i], hidden[i+1], bias=False, binarize_fn=self.binarize),\n",
    "                nn.BatchNorm1d(hidden[i+1]),\n",
    "                nn.Hardtanh(),\n",
    "                nn.Dropout(p_drop),\n",
    "            ]\n",
    "        self.bin_stack = nn.Sequential(*blocks) if blocks else nn.Identity()\n",
    "        self.out = BinaryLinear(hidden[-1], 1, bias=True, binarize_fn=self.binarize)\n",
    "    def forward(self, x):\n",
    "        x = self.float1(x)\n",
    "        x = self.bin_stack(x)\n",
    "        return self.out(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797eba59",
   "metadata": {},
   "source": [
    "## 5) Train (unweighted BCE; batch sizes unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3900484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | val_loss=0.7668 acc=0.532 f1=0.473 auc=0.533\n",
      "Epoch 05 | val_loss=0.6741 acc=0.550 f1=0.487 auc=0.623\n",
      "Epoch 10 | val_loss=0.7054 acc=0.609 f1=0.559 auc=0.640\n",
      "Epoch 15 | val_loss=0.6224 acc=0.659 f1=0.638 auc=0.742\n",
      "Epoch 20 | val_loss=0.5734 acc=0.727 f1=0.721 auc=0.776\n",
      "Epoch 25 | val_loss=0.5360 acc=0.743 f1=0.743 auc=0.816\n",
      "Epoch 30 | val_loss=0.4514 acc=0.806 f1=0.814 auc=0.863\n",
      "Epoch 35 | val_loss=0.4438 acc=0.825 f1=0.832 auc=0.878\n",
      "Epoch 40 | val_loss=0.3665 acc=0.840 f1=0.841 auc=0.915\n",
      "Epoch 45 | val_loss=0.3751 acc=0.851 f1=0.856 auc=0.894\n",
      "Epoch 50 | val_loss=0.3432 acc=0.863 f1=0.875 auc=0.932\n",
      "Epoch 55 | val_loss=0.2984 acc=0.898 f1=0.904 auc=0.948\n",
      "Epoch 60 | val_loss=0.3310 acc=0.874 f1=0.880 auc=0.933\n",
      "Epoch 65 | val_loss=0.3252 acc=0.883 f1=0.890 auc=0.937\n",
      "Epoch 70 | val_loss=0.2821 acc=0.895 f1=0.903 auc=0.945\n",
      "Epoch 75 | val_loss=0.2874 acc=0.889 f1=0.893 auc=0.950\n",
      "Epoch 80 | val_loss=0.2430 acc=0.905 f1=0.907 auc=0.954\n",
      "Epoch 85 | val_loss=0.3047 acc=0.887 f1=0.893 auc=0.946\n",
      "Epoch 90 | val_loss=0.3194 acc=0.880 f1=0.884 auc=0.945\n",
      "Epoch 95 | val_loss=0.2837 acc=0.904 f1=0.909 auc=0.943\n",
      "Epoch 100 | val_loss=0.2693 acc=0.904 f1=0.910 auc=0.953\n",
      "Epoch 105 | val_loss=0.2789 acc=0.920 f1=0.924 auc=0.943\n",
      "Epoch 110 | val_loss=0.2682 acc=0.902 f1=0.909 auc=0.953\n",
      "Epoch 115 | val_loss=0.2605 acc=0.917 f1=0.923 auc=0.955\n",
      "Epoch 120 | val_loss=0.2471 acc=0.916 f1=0.920 auc=0.960\n",
      "Epoch 125 | val_loss=0.2527 acc=0.909 f1=0.914 auc=0.957\n",
      "Epoch 130 | val_loss=0.3161 acc=0.898 f1=0.905 auc=0.929\n",
      "Epoch 135 | val_loss=0.2746 acc=0.921 f1=0.925 auc=0.956\n",
      "Epoch 140 | val_loss=0.2298 acc=0.931 f1=0.934 auc=0.959\n",
      "Epoch 145 | val_loss=0.2579 acc=0.926 f1=0.931 auc=0.954\n",
      "Epoch 150 | val_loss=0.2088 acc=0.939 f1=0.943 auc=0.970\n",
      "Epoch 155 | val_loss=0.2122 acc=0.941 f1=0.944 auc=0.969\n",
      "Epoch 160 | val_loss=0.2075 acc=0.935 f1=0.938 auc=0.968\n",
      "Epoch 165 | val_loss=0.2252 acc=0.932 f1=0.935 auc=0.966\n",
      "Epoch 170 | val_loss=0.2479 acc=0.930 f1=0.934 auc=0.962\n",
      "Epoch 175 | val_loss=0.2507 acc=0.928 f1=0.933 auc=0.968\n",
      "Epoch 180 | val_loss=0.2520 acc=0.927 f1=0.932 auc=0.971\n",
      "Epoch 185 | val_loss=0.2346 acc=0.933 f1=0.936 auc=0.968\n",
      "Epoch 190 | val_loss=0.1860 acc=0.947 f1=0.950 auc=0.973\n",
      "Epoch 195 | val_loss=0.2507 acc=0.926 f1=0.931 auc=0.969\n",
      "Epoch 200 | val_loss=0.2754 acc=0.926 f1=0.930 auc=0.960\n",
      "Epoch 205 | val_loss=0.1909 acc=0.944 f1=0.947 auc=0.971\n",
      "Epoch 210 | val_loss=0.2219 acc=0.938 f1=0.942 auc=0.971\n",
      "Epoch 215 | val_loss=0.2636 acc=0.915 f1=0.921 auc=0.965\n",
      "Epoch 220 | val_loss=0.1749 acc=0.950 f1=0.953 auc=0.980\n",
      "Epoch 225 | val_loss=0.2330 acc=0.932 f1=0.936 auc=0.966\n",
      "Epoch 230 | val_loss=0.2319 acc=0.932 f1=0.935 auc=0.975\n",
      "Epoch 235 | val_loss=0.1681 acc=0.953 f1=0.955 auc=0.977\n",
      "Epoch 240 | val_loss=0.2060 acc=0.944 f1=0.947 auc=0.972\n",
      "Epoch 245 | val_loss=0.1851 acc=0.953 f1=0.955 auc=0.979\n",
      "Epoch 250 | val_loss=0.1974 acc=0.946 f1=0.949 auc=0.980\n",
      "Epoch 255 | val_loss=0.1471 acc=0.963 f1=0.964 auc=0.983\n",
      "Epoch 260 | val_loss=0.2142 acc=0.947 f1=0.950 auc=0.975\n",
      "Epoch 265 | val_loss=0.2300 acc=0.939 f1=0.942 auc=0.978\n",
      "Epoch 270 | val_loss=0.2092 acc=0.939 f1=0.942 auc=0.979\n",
      "Epoch 275 | val_loss=0.2390 acc=0.934 f1=0.938 auc=0.965\n",
      "Epoch 280 | val_loss=0.2263 acc=0.932 f1=0.935 auc=0.977\n",
      "Epoch 285 | val_loss=0.2361 acc=0.941 f1=0.944 auc=0.973\n",
      "Epoch 290 | val_loss=0.1638 acc=0.953 f1=0.955 auc=0.981\n",
      "Epoch 295 | val_loss=0.2440 acc=0.937 f1=0.941 auc=0.974\n",
      "Epoch 300 | val_loss=0.1717 acc=0.950 f1=0.953 auc=0.985\n",
      "Epoch 305 | val_loss=0.1977 acc=0.950 f1=0.953 auc=0.978\n",
      "Epoch 310 | val_loss=0.2335 acc=0.943 f1=0.945 auc=0.973\n",
      "Epoch 315 | val_loss=0.1860 acc=0.944 f1=0.947 auc=0.979\n",
      "Epoch 320 | val_loss=0.2334 acc=0.947 f1=0.950 auc=0.979\n",
      "Epoch 325 | val_loss=0.1740 acc=0.958 f1=0.960 auc=0.978\n",
      "Epoch 330 | val_loss=0.1775 acc=0.956 f1=0.958 auc=0.982\n",
      "Epoch 335 | val_loss=0.1325 acc=0.961 f1=0.963 auc=0.986\n",
      "Epoch 340 | val_loss=0.1584 acc=0.956 f1=0.958 auc=0.985\n",
      "Epoch 345 | val_loss=0.1726 acc=0.953 f1=0.954 auc=0.981\n",
      "Epoch 350 | val_loss=0.1818 acc=0.963 f1=0.964 auc=0.987\n",
      "Epoch 355 | val_loss=0.2211 acc=0.941 f1=0.944 auc=0.978\n",
      "Epoch 360 | val_loss=0.1642 acc=0.963 f1=0.964 auc=0.981\n",
      "Epoch 365 | val_loss=0.2198 acc=0.957 f1=0.959 auc=0.975\n",
      "Epoch 370 | val_loss=0.1603 acc=0.944 f1=0.946 auc=0.979\n",
      "Epoch 375 | val_loss=0.1750 acc=0.960 f1=0.962 auc=0.982\n",
      "Epoch 380 | val_loss=0.1963 acc=0.963 f1=0.964 auc=0.977\n",
      "Epoch 385 | val_loss=0.2160 acc=0.954 f1=0.956 auc=0.979\n",
      "Epoch 390 | val_loss=0.1891 acc=0.957 f1=0.959 auc=0.984\n",
      "Epoch 395 | val_loss=0.1771 acc=0.957 f1=0.959 auc=0.980\n",
      "Epoch 400 | val_loss=0.1593 acc=0.956 f1=0.958 auc=0.982\n",
      "Epoch 405 | val_loss=0.1766 acc=0.964 f1=0.965 auc=0.980\n",
      "Epoch 410 | val_loss=0.1658 acc=0.958 f1=0.960 auc=0.982\n",
      "Epoch 415 | val_loss=0.2590 acc=0.950 f1=0.952 auc=0.975\n",
      "Epoch 420 | val_loss=0.1611 acc=0.953 f1=0.955 auc=0.986\n",
      "Epoch 425 | val_loss=0.2184 acc=0.947 f1=0.950 auc=0.979\n",
      "Epoch 430 | val_loss=0.1909 acc=0.958 f1=0.960 auc=0.986\n",
      "Epoch 435 | val_loss=0.1583 acc=0.960 f1=0.962 auc=0.987\n",
      "Epoch 440 | val_loss=0.1751 acc=0.952 f1=0.953 auc=0.982\n",
      "Epoch 445 | val_loss=0.1622 acc=0.974 f1=0.974 auc=0.987\n",
      "Epoch 450 | val_loss=0.1767 acc=0.956 f1=0.958 auc=0.987\n",
      "Epoch 455 | val_loss=0.1914 acc=0.952 f1=0.954 auc=0.974\n",
      "Epoch 460 | val_loss=0.1136 acc=0.968 f1=0.969 auc=0.991\n",
      "Epoch 465 | val_loss=0.1483 acc=0.964 f1=0.965 auc=0.982\n",
      "Epoch 470 | val_loss=0.1096 acc=0.968 f1=0.969 auc=0.989\n",
      "Epoch 475 | val_loss=0.1961 acc=0.963 f1=0.964 auc=0.985\n",
      "Epoch 480 | val_loss=0.1700 acc=0.956 f1=0.957 auc=0.987\n",
      "Epoch 485 | val_loss=0.1833 acc=0.968 f1=0.969 auc=0.987\n",
      "Epoch 490 | val_loss=0.2027 acc=0.955 f1=0.956 auc=0.981\n",
      "Epoch 495 | val_loss=0.1820 acc=0.966 f1=0.967 auc=0.986\n",
      "Epoch 500 | val_loss=0.1673 acc=0.953 f1=0.955 auc=0.980\n",
      "Epoch 505 | val_loss=0.1374 acc=0.970 f1=0.971 auc=0.987\n",
      "Epoch 510 | val_loss=0.1769 acc=0.964 f1=0.965 auc=0.988\n",
      "Epoch 515 | val_loss=0.1900 acc=0.954 f1=0.956 auc=0.984\n",
      "Epoch 520 | val_loss=0.1554 acc=0.963 f1=0.964 auc=0.988\n",
      "Epoch 525 | val_loss=0.1584 acc=0.968 f1=0.969 auc=0.989\n",
      "Epoch 530 | val_loss=0.2009 acc=0.958 f1=0.960 auc=0.982\n",
      "Epoch 535 | val_loss=0.1168 acc=0.968 f1=0.969 auc=0.986\n",
      "Epoch 540 | val_loss=0.1452 acc=0.966 f1=0.967 auc=0.993\n",
      "Epoch 545 | val_loss=0.1154 acc=0.968 f1=0.969 auc=0.990\n",
      "Epoch 550 | val_loss=0.1430 acc=0.960 f1=0.962 auc=0.989\n",
      "Epoch 555 | val_loss=0.1423 acc=0.960 f1=0.962 auc=0.988\n",
      "Epoch 560 | val_loss=0.1415 acc=0.967 f1=0.968 auc=0.988\n",
      "Epoch 565 | val_loss=0.2466 acc=0.957 f1=0.959 auc=0.983\n",
      "Epoch 570 | val_loss=0.1332 acc=0.964 f1=0.965 auc=0.988\n",
      "Epoch 575 | val_loss=0.1455 acc=0.965 f1=0.966 auc=0.990\n",
      "Epoch 580 | val_loss=0.1090 acc=0.967 f1=0.968 auc=0.990\n",
      "Epoch 585 | val_loss=0.2036 acc=0.964 f1=0.965 auc=0.989\n",
      "Epoch 590 | val_loss=0.1516 acc=0.971 f1=0.972 auc=0.988\n",
      "Epoch 595 | val_loss=0.0969 acc=0.978 f1=0.978 auc=0.990\n",
      "Epoch 600 | val_loss=0.1012 acc=0.978 f1=0.978 auc=0.988\n",
      "Epoch 605 | val_loss=0.1337 acc=0.975 f1=0.975 auc=0.988\n",
      "Epoch 610 | val_loss=0.1506 acc=0.968 f1=0.969 auc=0.987\n",
      "Epoch 615 | val_loss=0.2026 acc=0.971 f1=0.972 auc=0.988\n",
      "Epoch 620 | val_loss=0.1623 acc=0.969 f1=0.970 auc=0.988\n",
      "Epoch 625 | val_loss=0.1193 acc=0.963 f1=0.964 auc=0.994\n",
      "Epoch 630 | val_loss=0.1353 acc=0.969 f1=0.970 auc=0.989\n",
      "Epoch 635 | val_loss=0.1208 acc=0.966 f1=0.967 auc=0.990\n",
      "Epoch 640 | val_loss=0.1097 acc=0.969 f1=0.970 auc=0.987\n",
      "Epoch 645 | val_loss=0.1117 acc=0.971 f1=0.972 auc=0.992\n",
      "Epoch 650 | val_loss=0.1505 acc=0.972 f1=0.973 auc=0.991\n",
      "Epoch 655 | val_loss=0.1243 acc=0.971 f1=0.972 auc=0.988\n",
      "Epoch 660 | val_loss=0.1056 acc=0.971 f1=0.972 auc=0.992\n",
      "Epoch 665 | val_loss=0.1661 acc=0.969 f1=0.970 auc=0.985\n",
      "Epoch 670 | val_loss=0.1741 acc=0.963 f1=0.964 auc=0.982\n",
      "Epoch 675 | val_loss=0.1019 acc=0.971 f1=0.972 auc=0.992\n",
      "Epoch 680 | val_loss=0.1037 acc=0.977 f1=0.977 auc=0.989\n",
      "Epoch 685 | val_loss=0.1437 acc=0.971 f1=0.972 auc=0.990\n",
      "Epoch 690 | val_loss=0.1823 acc=0.961 f1=0.963 auc=0.988\n",
      "Epoch 695 | val_loss=0.1382 acc=0.966 f1=0.967 auc=0.989\n",
      "Epoch 700 | val_loss=0.2047 acc=0.959 f1=0.961 auc=0.987\n",
      "Epoch 705 | val_loss=0.1314 acc=0.967 f1=0.968 auc=0.983\n",
      "Epoch 710 | val_loss=0.1599 acc=0.963 f1=0.964 auc=0.988\n",
      "Epoch 715 | val_loss=0.1245 acc=0.968 f1=0.969 auc=0.990\n",
      "Epoch 720 | val_loss=0.1467 acc=0.966 f1=0.967 auc=0.988\n",
      "Epoch 725 | val_loss=0.1375 acc=0.970 f1=0.971 auc=0.989\n",
      "Epoch 730 | val_loss=0.1156 acc=0.976 f1=0.976 auc=0.994\n",
      "Epoch 735 | val_loss=0.1352 acc=0.961 f1=0.963 auc=0.990\n",
      "Epoch 740 | val_loss=0.1339 acc=0.965 f1=0.966 auc=0.990\n",
      "Epoch 745 | val_loss=0.1809 acc=0.970 f1=0.971 auc=0.991\n",
      "Epoch 750 | val_loss=0.1093 acc=0.974 f1=0.974 auc=0.995\n",
      "Epoch 755 | val_loss=0.1107 acc=0.971 f1=0.972 auc=0.989\n",
      "Epoch 760 | val_loss=0.1319 acc=0.966 f1=0.967 auc=0.985\n",
      "Epoch 765 | val_loss=0.1839 acc=0.961 f1=0.963 auc=0.993\n",
      "Epoch 770 | val_loss=0.1541 acc=0.967 f1=0.968 auc=0.992\n",
      "Epoch 775 | val_loss=0.1158 acc=0.974 f1=0.974 auc=0.988\n",
      "Epoch 780 | val_loss=0.1234 acc=0.974 f1=0.974 auc=0.991\n",
      "Epoch 785 | val_loss=0.1030 acc=0.976 f1=0.976 auc=0.991\n",
      "Epoch 790 | val_loss=0.1427 acc=0.968 f1=0.969 auc=0.987\n",
      "Epoch 795 | val_loss=0.1612 acc=0.968 f1=0.969 auc=0.986\n",
      "Epoch 800 | val_loss=0.1857 acc=0.974 f1=0.974 auc=0.989\n",
      "Epoch 805 | val_loss=0.1364 acc=0.966 f1=0.967 auc=0.986\n",
      "Epoch 810 | val_loss=0.1162 acc=0.974 f1=0.974 auc=0.990\n",
      "Epoch 815 | val_loss=0.1775 acc=0.970 f1=0.971 auc=0.985\n",
      "Epoch 820 | val_loss=0.1317 acc=0.974 f1=0.974 auc=0.988\n",
      "Epoch 825 | val_loss=0.1671 acc=0.964 f1=0.965 auc=0.990\n",
      "Epoch 830 | val_loss=0.1485 acc=0.971 f1=0.972 auc=0.985\n",
      "Epoch 835 | val_loss=0.0893 acc=0.979 f1=0.979 auc=0.992\n",
      "Epoch 840 | val_loss=0.1464 acc=0.975 f1=0.975 auc=0.990\n",
      "Epoch 845 | val_loss=0.1373 acc=0.971 f1=0.972 auc=0.993\n",
      "Epoch 850 | val_loss=0.0952 acc=0.977 f1=0.977 auc=0.990\n",
      "Epoch 855 | val_loss=0.1529 acc=0.969 f1=0.970 auc=0.994\n",
      "Epoch 860 | val_loss=0.1969 acc=0.967 f1=0.968 auc=0.987\n",
      "Epoch 865 | val_loss=0.1173 acc=0.980 f1=0.981 auc=0.985\n",
      "Epoch 870 | val_loss=0.1896 acc=0.968 f1=0.969 auc=0.985\n",
      "Epoch 875 | val_loss=0.1288 acc=0.976 f1=0.976 auc=0.992\n",
      "Epoch 880 | val_loss=0.1467 acc=0.969 f1=0.970 auc=0.987\n",
      "Epoch 885 | val_loss=0.1626 acc=0.969 f1=0.970 auc=0.986\n",
      "Epoch 890 | val_loss=0.1457 acc=0.966 f1=0.967 auc=0.988\n",
      "Epoch 895 | val_loss=0.1184 acc=0.969 f1=0.970 auc=0.987\n",
      "Epoch 900 | val_loss=0.2003 acc=0.966 f1=0.967 auc=0.988\n",
      "Epoch 905 | val_loss=0.1611 acc=0.967 f1=0.968 auc=0.987\n",
      "Epoch 910 | val_loss=0.1622 acc=0.969 f1=0.970 auc=0.991\n",
      "Epoch 915 | val_loss=0.1352 acc=0.970 f1=0.971 auc=0.989\n",
      "Epoch 920 | val_loss=0.1501 acc=0.975 f1=0.975 auc=0.988\n",
      "Epoch 925 | val_loss=0.1799 acc=0.964 f1=0.965 auc=0.983\n",
      "Epoch 930 | val_loss=0.0846 acc=0.981 f1=0.982 auc=0.992\n",
      "Epoch 935 | val_loss=0.1680 acc=0.969 f1=0.970 auc=0.987\n",
      "Epoch 940 | val_loss=0.1467 acc=0.977 f1=0.977 auc=0.986\n",
      "Epoch 945 | val_loss=0.1225 acc=0.971 f1=0.972 auc=0.989\n",
      "Epoch 950 | val_loss=0.1755 acc=0.974 f1=0.974 auc=0.988\n",
      "Epoch 955 | val_loss=0.2029 acc=0.965 f1=0.966 auc=0.984\n",
      "Epoch 960 | val_loss=0.1700 acc=0.968 f1=0.969 auc=0.984\n",
      "Epoch 965 | val_loss=0.1252 acc=0.969 f1=0.970 auc=0.981\n",
      "Epoch 970 | val_loss=0.1677 acc=0.966 f1=0.967 auc=0.988\n",
      "Epoch 975 | val_loss=0.2293 acc=0.958 f1=0.960 auc=0.982\n",
      "Epoch 980 | val_loss=0.1546 acc=0.976 f1=0.976 auc=0.989\n",
      "Epoch 985 | val_loss=0.1924 acc=0.965 f1=0.966 auc=0.983\n",
      "Epoch 990 | val_loss=0.1697 acc=0.970 f1=0.971 auc=0.984\n",
      "Epoch 995 | val_loss=0.1259 acc=0.968 f1=0.969 auc=0.988\n",
      "Epoch 1000 | val_loss=0.1761 acc=0.968 f1=0.969 auc=0.987\n",
      "Epoch 1005 | val_loss=0.1753 acc=0.967 f1=0.968 auc=0.981\n",
      "Epoch 1010 | val_loss=0.1740 acc=0.969 f1=0.970 auc=0.985\n",
      "Epoch 1015 | val_loss=0.1861 acc=0.965 f1=0.966 auc=0.986\n",
      "Epoch 1020 | val_loss=0.1875 acc=0.963 f1=0.964 auc=0.987\n",
      "Epoch 1025 | val_loss=0.1625 acc=0.966 f1=0.967 auc=0.990\n",
      "Epoch 1030 | val_loss=0.1690 acc=0.972 f1=0.973 auc=0.986\n",
      "Epoch 1035 | val_loss=0.2212 acc=0.965 f1=0.966 auc=0.987\n",
      "Epoch 1040 | val_loss=0.0984 acc=0.977 f1=0.977 auc=0.990\n",
      "Epoch 1045 | val_loss=0.1506 acc=0.970 f1=0.971 auc=0.987\n",
      "Epoch 1050 | val_loss=0.2419 acc=0.963 f1=0.964 auc=0.981\n",
      "Epoch 1055 | val_loss=0.2128 acc=0.974 f1=0.974 auc=0.986\n",
      "Epoch 1060 | val_loss=0.1993 acc=0.964 f1=0.965 auc=0.986\n",
      "Epoch 1065 | val_loss=0.1542 acc=0.970 f1=0.971 auc=0.987\n",
      "Epoch 1070 | val_loss=0.1476 acc=0.967 f1=0.968 auc=0.985\n",
      "Epoch 1075 | val_loss=0.1767 acc=0.968 f1=0.969 auc=0.982\n",
      "Epoch 1080 | val_loss=0.1464 acc=0.967 f1=0.968 auc=0.987\n",
      "Epoch 1085 | val_loss=0.1719 acc=0.969 f1=0.970 auc=0.986\n",
      "Epoch 1090 | val_loss=0.1155 acc=0.976 f1=0.976 auc=0.991\n",
      "Epoch 1095 | val_loss=0.1430 acc=0.977 f1=0.977 auc=0.985\n",
      "Epoch 1100 | val_loss=0.2511 acc=0.963 f1=0.964 auc=0.980\n",
      "Epoch 1105 | val_loss=0.2206 acc=0.966 f1=0.967 auc=0.983\n",
      "Epoch 1110 | val_loss=0.1698 acc=0.969 f1=0.970 auc=0.982\n",
      "Epoch 1115 | val_loss=0.1321 acc=0.970 f1=0.971 auc=0.981\n",
      "Epoch 1120 | val_loss=0.1219 acc=0.974 f1=0.974 auc=0.989\n",
      "Epoch 1125 | val_loss=0.1581 acc=0.971 f1=0.972 auc=0.988\n",
      "Epoch 1130 | val_loss=0.1312 acc=0.976 f1=0.976 auc=0.989\n",
      "Epoch 1135 | val_loss=0.1502 acc=0.976 f1=0.976 auc=0.990\n",
      "Epoch 1140 | val_loss=0.1711 acc=0.974 f1=0.974 auc=0.990\n",
      "Epoch 1145 | val_loss=0.1848 acc=0.966 f1=0.967 auc=0.989\n",
      "Epoch 1150 | val_loss=0.1286 acc=0.976 f1=0.976 auc=0.986\n",
      "Epoch 1155 | val_loss=0.1896 acc=0.961 f1=0.963 auc=0.983\n",
      "Epoch 1160 | val_loss=0.2189 acc=0.967 f1=0.968 auc=0.983\n",
      "Epoch 1165 | val_loss=0.1928 acc=0.971 f1=0.972 auc=0.990\n",
      "Epoch 1170 | val_loss=0.1656 acc=0.967 f1=0.968 auc=0.989\n",
      "Epoch 1175 | val_loss=0.1014 acc=0.974 f1=0.974 auc=0.994\n",
      "Epoch 1180 | val_loss=0.1101 acc=0.980 f1=0.981 auc=0.991\n",
      "Epoch 1185 | val_loss=0.1776 acc=0.972 f1=0.973 auc=0.988\n",
      "Epoch 1190 | val_loss=0.1394 acc=0.975 f1=0.975 auc=0.989\n",
      "Epoch 1195 | val_loss=0.1446 acc=0.968 f1=0.969 auc=0.994\n",
      "Epoch 1200 | val_loss=0.1258 acc=0.980 f1=0.981 auc=0.993\n",
      "Epoch 1205 | val_loss=0.1573 acc=0.975 f1=0.975 auc=0.988\n",
      "Epoch 1210 | val_loss=0.1622 acc=0.974 f1=0.974 auc=0.988\n",
      "Epoch 1215 | val_loss=0.1272 acc=0.980 f1=0.981 auc=0.990\n",
      "Epoch 1220 | val_loss=0.1881 acc=0.969 f1=0.970 auc=0.988\n",
      "Epoch 1225 | val_loss=0.1529 acc=0.967 f1=0.968 auc=0.987\n",
      "Epoch 1230 | val_loss=0.1596 acc=0.968 f1=0.969 auc=0.990\n",
      "Epoch 1235 | val_loss=0.1776 acc=0.971 f1=0.972 auc=0.989\n",
      "Epoch 1240 | val_loss=0.1384 acc=0.968 f1=0.969 auc=0.992\n",
      "Epoch 1245 | val_loss=0.1854 acc=0.963 f1=0.964 auc=0.979\n",
      "Epoch 1250 | val_loss=0.1127 acc=0.976 f1=0.976 auc=0.992\n",
      "Epoch 1255 | val_loss=0.1493 acc=0.975 f1=0.975 auc=0.989\n",
      "Epoch 1260 | val_loss=0.1725 acc=0.970 f1=0.971 auc=0.986\n",
      "Epoch 1265 | val_loss=0.1718 acc=0.975 f1=0.975 auc=0.988\n",
      "Epoch 1270 | val_loss=0.1207 acc=0.976 f1=0.976 auc=0.990\n",
      "Epoch 1275 | val_loss=0.0832 acc=0.980 f1=0.981 auc=0.994\n",
      "Epoch 1280 | val_loss=0.1527 acc=0.972 f1=0.973 auc=0.989\n",
      "Epoch 1285 | val_loss=0.1502 acc=0.971 f1=0.972 auc=0.986\n",
      "Epoch 1290 | val_loss=0.1101 acc=0.970 f1=0.971 auc=0.988\n",
      "Epoch 1295 | val_loss=0.1328 acc=0.972 f1=0.973 auc=0.987\n",
      "Epoch 1300 | val_loss=0.1746 acc=0.974 f1=0.974 auc=0.993\n",
      "Epoch 1305 | val_loss=0.1537 acc=0.976 f1=0.976 auc=0.989\n",
      "Epoch 1310 | val_loss=0.1658 acc=0.976 f1=0.976 auc=0.989\n",
      "Epoch 1315 | val_loss=0.1904 acc=0.975 f1=0.975 auc=0.984\n",
      "Epoch 1320 | val_loss=0.1833 acc=0.972 f1=0.973 auc=0.986\n",
      "Epoch 1325 | val_loss=0.1202 acc=0.974 f1=0.974 auc=0.986\n",
      "Epoch 1330 | val_loss=0.1790 acc=0.968 f1=0.969 auc=0.987\n",
      "Epoch 1335 | val_loss=0.1852 acc=0.968 f1=0.969 auc=0.987\n",
      "Epoch 1340 | val_loss=0.1934 acc=0.968 f1=0.969 auc=0.988\n",
      "Epoch 1345 | val_loss=0.1838 acc=0.975 f1=0.975 auc=0.989\n",
      "Epoch 1350 | val_loss=0.2210 acc=0.967 f1=0.968 auc=0.987\n",
      "Epoch 1355 | val_loss=0.1699 acc=0.975 f1=0.975 auc=0.987\n",
      "Epoch 1360 | val_loss=0.2023 acc=0.960 f1=0.962 auc=0.984\n",
      "Epoch 1365 | val_loss=0.1585 acc=0.969 f1=0.970 auc=0.989\n",
      "Epoch 1370 | val_loss=0.1546 acc=0.969 f1=0.970 auc=0.989\n",
      "Epoch 1375 | val_loss=0.1553 acc=0.970 f1=0.971 auc=0.986\n",
      "Epoch 1380 | val_loss=0.1396 acc=0.971 f1=0.972 auc=0.991\n",
      "Epoch 1385 | val_loss=0.1619 acc=0.970 f1=0.971 auc=0.992\n",
      "Epoch 1390 | val_loss=0.1792 acc=0.972 f1=0.973 auc=0.990\n",
      "Epoch 1395 | val_loss=0.1484 acc=0.975 f1=0.975 auc=0.989\n",
      "Epoch 1400 | val_loss=0.1561 acc=0.966 f1=0.967 auc=0.989\n",
      "Epoch 1405 | val_loss=0.1589 acc=0.970 f1=0.971 auc=0.991\n",
      "Epoch 1410 | val_loss=0.1942 acc=0.966 f1=0.967 auc=0.990\n",
      "Epoch 1415 | val_loss=0.1819 acc=0.961 f1=0.963 auc=0.993\n",
      "Epoch 1420 | val_loss=0.1784 acc=0.966 f1=0.967 auc=0.987\n",
      "Epoch 1425 | val_loss=0.1302 acc=0.977 f1=0.977 auc=0.991\n",
      "Epoch 1430 | val_loss=0.2035 acc=0.970 f1=0.971 auc=0.987\n",
      "Epoch 1435 | val_loss=0.1704 acc=0.976 f1=0.976 auc=0.987\n",
      "Epoch 1440 | val_loss=0.1513 acc=0.981 f1=0.982 auc=0.988\n",
      "Epoch 1445 | val_loss=0.0969 acc=0.982 f1=0.983 auc=0.993\n",
      "Epoch 1450 | val_loss=0.1719 acc=0.970 f1=0.971 auc=0.989\n",
      "Epoch 1455 | val_loss=0.1099 acc=0.978 f1=0.978 auc=0.991\n",
      "Epoch 1460 | val_loss=0.1674 acc=0.974 f1=0.974 auc=0.992\n",
      "Epoch 1465 | val_loss=0.1824 acc=0.968 f1=0.969 auc=0.989\n",
      "Epoch 1470 | val_loss=0.2840 acc=0.964 f1=0.965 auc=0.985\n",
      "Epoch 1475 | val_loss=0.1299 acc=0.971 f1=0.972 auc=0.993\n",
      "Epoch 1480 | val_loss=0.1397 acc=0.968 f1=0.969 auc=0.994\n",
      "Epoch 1485 | val_loss=0.1819 acc=0.975 f1=0.975 auc=0.990\n",
      "Epoch 1490 | val_loss=0.1358 acc=0.970 f1=0.971 auc=0.994\n",
      "Epoch 1495 | val_loss=0.1551 acc=0.967 f1=0.968 auc=0.992\n",
      "Epoch 1500 | val_loss=0.1780 acc=0.970 f1=0.971 auc=0.988\n",
      "Epoch 1505 | val_loss=0.1272 acc=0.980 f1=0.981 auc=0.990\n",
      "Epoch 1510 | val_loss=0.1221 acc=0.981 f1=0.982 auc=0.994\n",
      "Epoch 1515 | val_loss=0.1354 acc=0.967 f1=0.968 auc=0.994\n",
      "Epoch 1520 | val_loss=0.1556 acc=0.966 f1=0.967 auc=0.987\n",
      "Epoch 1525 | val_loss=0.1879 acc=0.970 f1=0.971 auc=0.989\n",
      "Epoch 1530 | val_loss=0.1402 acc=0.977 f1=0.977 auc=0.988\n",
      "Epoch 1535 | val_loss=0.2015 acc=0.970 f1=0.971 auc=0.992\n",
      "Epoch 1540 | val_loss=0.2237 acc=0.969 f1=0.970 auc=0.989\n",
      "Epoch 1545 | val_loss=0.1606 acc=0.974 f1=0.974 auc=0.992\n",
      "Epoch 1550 | val_loss=0.1316 acc=0.976 f1=0.976 auc=0.993\n",
      "Epoch 1555 | val_loss=0.1677 acc=0.970 f1=0.971 auc=0.993\n",
      "Epoch 1560 | val_loss=0.1110 acc=0.976 f1=0.976 auc=0.995\n",
      "Epoch 1565 | val_loss=0.0986 acc=0.979 f1=0.980 auc=0.992\n",
      "Epoch 1570 | val_loss=0.1134 acc=0.977 f1=0.977 auc=0.988\n",
      "Epoch 1575 | val_loss=0.1971 acc=0.977 f1=0.977 auc=0.988\n",
      "Epoch 1580 | val_loss=0.1789 acc=0.975 f1=0.975 auc=0.990\n",
      "Epoch 1585 | val_loss=0.1254 acc=0.978 f1=0.978 auc=0.986\n",
      "Epoch 1590 | val_loss=0.1409 acc=0.972 f1=0.973 auc=0.990\n",
      "Epoch 1595 | val_loss=0.1258 acc=0.974 f1=0.974 auc=0.990\n",
      "Epoch 1600 | val_loss=0.1607 acc=0.975 f1=0.975 auc=0.991\n",
      "Epoch 1605 | val_loss=0.1466 acc=0.972 f1=0.973 auc=0.991\n",
      "Epoch 1610 | val_loss=0.1914 acc=0.969 f1=0.970 auc=0.988\n",
      "Epoch 1615 | val_loss=0.1843 acc=0.970 f1=0.971 auc=0.987\n",
      "Epoch 1620 | val_loss=0.2196 acc=0.971 f1=0.972 auc=0.989\n",
      "Epoch 1625 | val_loss=0.1595 acc=0.972 f1=0.973 auc=0.993\n",
      "Epoch 1630 | val_loss=0.1651 acc=0.970 f1=0.971 auc=0.989\n",
      "Epoch 1635 | val_loss=0.1372 acc=0.976 f1=0.976 auc=0.988\n",
      "Epoch 1640 | val_loss=0.1835 acc=0.970 f1=0.971 auc=0.991\n",
      "Epoch 1645 | val_loss=0.1420 acc=0.970 f1=0.971 auc=0.990\n",
      "Epoch 1650 | val_loss=0.1732 acc=0.966 f1=0.967 auc=0.987\n",
      "Epoch 1655 | val_loss=0.1369 acc=0.977 f1=0.977 auc=0.987\n",
      "Epoch 1660 | val_loss=0.1958 acc=0.965 f1=0.966 auc=0.989\n",
      "Epoch 1665 | val_loss=0.1594 acc=0.971 f1=0.972 auc=0.991\n",
      "Epoch 1670 | val_loss=0.1710 acc=0.979 f1=0.980 auc=0.989\n",
      "Epoch 1675 | val_loss=0.1140 acc=0.979 f1=0.980 auc=0.992\n",
      "Epoch 1680 | val_loss=0.1024 acc=0.982 f1=0.983 auc=0.993\n",
      "Epoch 1685 | val_loss=0.2160 acc=0.957 f1=0.959 auc=0.987\n",
      "Epoch 1690 | val_loss=0.1098 acc=0.976 f1=0.976 auc=0.988\n",
      "Epoch 1695 | val_loss=0.1459 acc=0.972 f1=0.973 auc=0.991\n",
      "Epoch 1700 | val_loss=0.2594 acc=0.968 f1=0.969 auc=0.988\n",
      "Epoch 1705 | val_loss=0.1989 acc=0.972 f1=0.973 auc=0.988\n",
      "Epoch 1710 | val_loss=0.0952 acc=0.985 f1=0.985 auc=0.989\n",
      "Epoch 1715 | val_loss=0.1620 acc=0.974 f1=0.974 auc=0.994\n",
      "Epoch 1720 | val_loss=0.1982 acc=0.970 f1=0.971 auc=0.990\n",
      "Epoch 1725 | val_loss=0.1625 acc=0.979 f1=0.980 auc=0.992\n",
      "Epoch 1730 | val_loss=0.1691 acc=0.969 f1=0.970 auc=0.987\n",
      "Epoch 1735 | val_loss=0.1055 acc=0.982 f1=0.983 auc=0.993\n",
      "Epoch 1740 | val_loss=0.1374 acc=0.977 f1=0.977 auc=0.993\n",
      "Epoch 1745 | val_loss=0.1016 acc=0.978 f1=0.978 auc=0.994\n",
      "Epoch 1750 | val_loss=0.1184 acc=0.982 f1=0.983 auc=0.990\n",
      "Epoch 1755 | val_loss=0.1257 acc=0.976 f1=0.976 auc=0.993\n",
      "Epoch 1760 | val_loss=0.1140 acc=0.977 f1=0.977 auc=0.992\n",
      "Epoch 1765 | val_loss=0.0976 acc=0.978 f1=0.978 auc=0.995\n",
      "Epoch 1770 | val_loss=0.1541 acc=0.976 f1=0.976 auc=0.994\n",
      "Epoch 1775 | val_loss=0.1079 acc=0.976 f1=0.976 auc=0.996\n",
      "Epoch 1780 | val_loss=0.1257 acc=0.979 f1=0.980 auc=0.994\n",
      "Epoch 1785 | val_loss=0.1117 acc=0.981 f1=0.982 auc=0.994\n",
      "Epoch 1790 | val_loss=0.1251 acc=0.983 f1=0.984 auc=0.994\n",
      "Epoch 1795 | val_loss=0.0885 acc=0.985 f1=0.985 auc=0.994\n",
      "Epoch 1800 | val_loss=0.1114 acc=0.978 f1=0.978 auc=0.993\n",
      "Epoch 1805 | val_loss=0.1194 acc=0.979 f1=0.980 auc=0.991\n",
      "Epoch 1810 | val_loss=0.1469 acc=0.975 f1=0.975 auc=0.994\n",
      "Epoch 1815 | val_loss=0.1410 acc=0.979 f1=0.979 auc=0.992\n",
      "Epoch 1820 | val_loss=0.1285 acc=0.982 f1=0.983 auc=0.992\n",
      "Epoch 1825 | val_loss=0.1132 acc=0.979 f1=0.980 auc=0.992\n",
      "Epoch 1830 | val_loss=0.1377 acc=0.981 f1=0.982 auc=0.989\n",
      "Epoch 1835 | val_loss=0.1422 acc=0.978 f1=0.978 auc=0.989\n",
      "Epoch 1840 | val_loss=0.1732 acc=0.969 f1=0.970 auc=0.990\n",
      "Epoch 1845 | val_loss=0.1000 acc=0.985 f1=0.985 auc=0.993\n",
      "Epoch 1850 | val_loss=0.1193 acc=0.978 f1=0.978 auc=0.994\n",
      "Epoch 1855 | val_loss=0.1559 acc=0.970 f1=0.971 auc=0.995\n",
      "Epoch 1860 | val_loss=0.1564 acc=0.972 f1=0.973 auc=0.990\n",
      "Epoch 1865 | val_loss=0.1336 acc=0.974 f1=0.974 auc=0.995\n",
      "Epoch 1870 | val_loss=0.1448 acc=0.979 f1=0.980 auc=0.993\n",
      "Epoch 1875 | val_loss=0.1114 acc=0.987 f1=0.987 auc=0.995\n",
      "Epoch 1880 | val_loss=0.1533 acc=0.970 f1=0.971 auc=0.993\n",
      "Epoch 1885 | val_loss=0.1219 acc=0.980 f1=0.981 auc=0.989\n",
      "Epoch 1890 | val_loss=0.1594 acc=0.976 f1=0.976 auc=0.988\n",
      "Epoch 1895 | val_loss=0.1033 acc=0.986 f1=0.986 auc=0.996\n",
      "Epoch 1900 | val_loss=0.2005 acc=0.961 f1=0.963 auc=0.990\n",
      "Epoch 1905 | val_loss=0.1389 acc=0.974 f1=0.974 auc=0.996\n",
      "Epoch 1910 | val_loss=0.1053 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 1915 | val_loss=0.0710 acc=0.985 f1=0.985 auc=0.996\n",
      "Epoch 1920 | val_loss=0.0372 acc=0.990 f1=0.990 auc=0.998\n",
      "Early stopping as acc >= 0.95\n",
      "Best @ epoch 1920 val_acc 0.99\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# DataLoaders (batch sizes unchanged)\n",
    "Xtr = torch.from_numpy(X_train_bal).to(device)\n",
    "ytr = torch.from_numpy(y_train_bal.astype(np.float32)).to(device)\n",
    "Xte = torch.from_numpy(X_test).to(device)\n",
    "yte = torch.from_numpy(y_test.astype(np.float32)).to(device)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr,ytr), batch_size=256, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(Xte,yte), batch_size=512, shuffle=False)\n",
    "\n",
    "model = HybridBNN(in_dim=X_train.shape[1]).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # unweighted loss\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, n = 0.0, 0\n",
    "    all_logits, all_y = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            total_loss += float(loss.item()) * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_y.append(yb.detach().cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    y_true = torch.cat(all_y).numpy()\n",
    "    probs = 1/(1+np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    return total_loss/max(1,n), acc, f1, auc\n",
    "\n",
    "best = {\"epoch\":-1, \"loss\":1e9, \"acc\":0.0, \"state\":None}\n",
    "EPOCHS = 3000\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "    val_loss, acc, f1, auc = evaluate()\n",
    "    if val_loss < best[\"loss\"]:\n",
    "        best.update({\"epoch\":epoch, \"loss\":val_loss, \"acc\":acc, \"state\":model.state_dict()})\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:02d} | val_loss={val_loss:.4f} acc={acc:.3f} f1={f1:.3f} auc={auc:.3f}\")\n",
    "    if acc >= 0.99:\n",
    "        print(\"Early stopping as acc >= 0.95\")\n",
    "        break\n",
    "model.load_state_dict(best[\"state\"])\n",
    "print(\"Best @ epoch\", best[\"epoch\"], \"val_acc\", round(best[\"acc\"],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fe526",
   "metadata": {},
   "source": [
    "## 6) Evaluation + Threshold Sweep (find a useful operating point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ecbd43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9802    0.9900       454\n",
      "           1     0.9806    1.0000    0.9902       454\n",
      "\n",
      "    accuracy                         0.9901       908\n",
      "   macro avg     0.9903    0.9901    0.9901       908\n",
      "weighted avg     0.9903    0.9901    0.9901       908\n",
      "\n",
      "ROC-AUC: 0.9976639368122805\n",
      "PR-AUC : 0.9954031376112874\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.9364721179008484 Best F1: 0.9923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9934    0.9912    0.9923       454\n",
      "           1     0.9912    0.9934    0.9923       454\n",
      "\n",
      "    accuracy                         0.9923       908\n",
      "   macro avg     0.9923    0.9923    0.9923       908\n",
      "weighted avg     0.9923    0.9923    0.9923       908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.from_numpy(X_test).to(device)).cpu().numpy()\n",
    "probs = 1/(1+np.exp(-logits))\n",
    "preds = (probs>=0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, preds, digits=4))\n",
    "try:\n",
    "    print('ROC-AUC:', roc_auc_score(y_test, probs))\n",
    "    print('PR-AUC :', average_precision_score(y_test, probs))\n",
    "except Exception as e:\n",
    "    print('AUC error:', e)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(); plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "plt.title('Confusion Matrix'); plt.colorbar();\n",
    "plt.xticks([0,1], ['No Asthma','Asthma']); plt.yticks([0,1], ['No Asthma','Asthma'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Threshold sweep (F1)\n",
    "prec, rec, thr = precision_recall_curve(y_test, probs)\n",
    "best_t, best_f1 = 0.5, -1.0\n",
    "for t in np.unique(np.clip(thr, 0, 1)):\n",
    "    pred = (probs >= t).astype(int)\n",
    "    f1 = f1_score(y_test, pred, zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_t = f1, float(t)\n",
    "print('Best threshold:', best_t, 'Best F1:', round(best_f1,4))\n",
    "print(classification_report(y_test, (probs>=best_t).astype(int), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70cbbfa",
   "metadata": {},
   "source": [
    "## 7) Save Artifacts (unchanged path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d8d5c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to bnn_artifacts\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path('bnn_artifacts')  # unchanged\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "torch.save(model.state_dict(), out_dir/'bnn_state.pt')\n",
    "with open(out_dir/'scaler.json','w') as f:\n",
    "    json.dump({'mean': x_mean.tolist(), 'scale': x_scale.tolist(), 'predictors': predictors, 'target': target_col}, f)\n",
    "print('Saved to', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29d1ff",
   "metadata": {},
   "source": [
    "## 8) Robust Prediction Helper (exact-forward; path auto-discovery inc. `/mnt/data/bnn_artifacts`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2b604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CANDIDATE_DIRS = ['bnn_artifacts','./bnn_artifacts','/mnt/data/bnn_artifacts', str(Path.cwd()/'bnn_artifacts')]\n",
    "_model_cache = {\"legacy\": None, \"z2p1\": None}\n",
    "\n",
    "def _find_art_dir():\n",
    "    for p in _CANDIDATE_DIRS:\n",
    "        d = Path(p)\n",
    "        if (d/'bnn_state.pt').exists() and (d/'scaler.json').exists():\n",
    "            return d\n",
    "    raise FileNotFoundError('Artifacts not found in: ' + ', '.join(_CANDIDATE_DIRS))\n",
    "\n",
    "def _infer_hidden_from_state(state, input_dim):\n",
    "    w2d = [(k,v) for k,v in state.items() if k.endswith('.weight') and hasattr(v,'ndim') and v.ndim==2]\n",
    "    hidden, first = [], False\n",
    "    for k,W in w2d:\n",
    "        out_f, in_f = W.shape\n",
    "        if not first:\n",
    "            if in_f != input_dim: continue\n",
    "            hidden.append(out_f); first = True; continue\n",
    "        if out_f == 1: break\n",
    "        hidden.append(out_f)\n",
    "    if not hidden: raise ValueError(\"Failed to infer hidden sizes from checkpoint.\")\n",
    "    return hidden\n",
    "\n",
    "def _build_bnn_from_state(state, input_dim, p_drop=0.1, binarize_mode=\"legacy\"):\n",
    "    hidden = _infer_hidden_from_state(state, input_dim)\n",
    "    m = HybridBNN(input_dim, hidden, p_drop=p_drop, binarize_mode=binarize_mode)\n",
    "    m._inferred_hidden = hidden\n",
    "    return m\n",
    "\n",
    "def _load_artifacts_for_infer(binarize_mode=\"legacy\"):\n",
    "    art = _find_art_dir()\n",
    "    meta = json.loads((art/'scaler.json').read_text())\n",
    "    preds  = meta['predictors']\n",
    "    x_mean = np.array(meta['mean'],  dtype=np.float32)\n",
    "    x_scale= np.array(meta['scale'], dtype=np.float32)\n",
    "    # honor metadata hint if present\n",
    "    bm = str(meta.get('binarize_mode', binarize_mode)).lower()\n",
    "    if bm in {\"legacy\",\"z2p1\"}: binarize_mode = bm\n",
    "    state = torch.load(art/'bnn_state.pt', map_location='cpu')\n",
    "    m = _build_bnn_from_state(state, input_dim=len(preds), p_drop=0.1, binarize_mode=binarize_mode)\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    m.eval()\n",
    "    print(f\"✅ Loaded from {art} | hidden sizes: {m._inferred_hidden} | binarize={binarize_mode}\")\n",
    "    return m, preds, x_mean, x_scale\n",
    "\n",
    "def _get_model(binarize_mode=\"legacy\"):\n",
    "    if _model_cache[binarize_mode] is None:\n",
    "        _model_cache[binarize_mode] = _load_artifacts_for_infer(binarize_mode)\n",
    "    return _model_cache[binarize_mode]\n",
    "\n",
    "def reset_infer_cache():\n",
    "    for k in _model_cache: _model_cache[k] = None\n",
    "\n",
    "# ---------- preprocessing & prediction ----------\n",
    "def _prep_df(df_like, predictors, x_mean, x_scale, strict=False):\n",
    "    if isinstance(df_like, dict):\n",
    "        df = pd.DataFrame([df_like])\n",
    "    elif isinstance(df_like, pd.Series):\n",
    "        df = pd.DataFrame([df_like.to_dict()])\n",
    "    else:\n",
    "        df = df_like.copy()\n",
    "\n",
    "    # enforce exact predictor order; fill missing with training mean (warn)\n",
    "    missing = [c for c in predictors if c not in df.columns]\n",
    "    if missing:\n",
    "        if strict:\n",
    "            raise KeyError(f\"Missing predictors: {missing}\")\n",
    "        print(f\"[WARN] Filling {len(missing)} missing column(s) with training mean: {missing[:8]}{'...' if len(missing)>8 else ''}\")\n",
    "        for c, mu in zip(predictors, x_mean):\n",
    "            if c not in df.columns:\n",
    "                df[c] = float(mu)\n",
    "\n",
    "    X = df[predictors].astype(np.float32).values\n",
    "    Xn = (X - x_mean) / (x_scale + 1e-8)\n",
    "    return Xn\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_asthma(df_like, threshold: float = 0.5, binarize_mode=\"legacy\", strict=False, verbose=False):\n",
    "    m, preds, mu, sigma = _get_model(binarize_mode)\n",
    "    Xn = _prep_df(df_like, preds, mu, sigma, strict=strict)\n",
    "    lg = m(torch.from_numpy(Xn)).cpu().numpy()\n",
    "    pr = 1/(1+np.exp(-lg))\n",
    "    lb = (pr >= float(threshold)).astype(np.int32)\n",
    "    if verbose:\n",
    "        print(f\"[{binarize_mode}] logits: min={lg.min():.4f} max={lg.max():.4f} mean={lg.mean():.4f}\")\n",
    "        print(f\"[{binarize_mode}] probs : min={pr.min():.4f} max={pr.max():.4f} mean={pr.mean():.4f}\")\n",
    "    return pr, lb, lg\n",
    "\n",
    "def compare_binarize_modes(df_like, threshold=0.5, strict=False):\n",
    "    out = {}\n",
    "    for mode in [\"legacy\",\"z2p1\"]:\n",
    "        try:\n",
    "            out[mode] = predict_asthma(df_like, threshold, binarize_mode=mode, strict=strict, verbose=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[{mode}] ERROR:\", e); out[mode]=None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537af8e",
   "metadata": {},
   "source": [
    "## 9) Parameter Size (KiB) — confirm ≤ 20KB total (fold BN for deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125ec47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded from bnn_artifacts | hidden sizes: [32, 64, 128, 128, 64, 64, 32] | binarize=legacy\n",
      "Estimated deployment size (binarize=legacy, BN=fold):\n",
      "  weight_kib: 7.7539 KiB\n",
      "  bias_kib: 0.0039 KiB\n",
      "  bn_kib: 0.0 KiB\n",
      "  total_kib: 7.7578 KiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weight_kib': 7.7539, 'bias_kib': 0.0039, 'bn_kib': 0.0, 'total_kib': 7.7578}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- size estimator ----------\n",
    "def param_size_kib(in_dim: int, hidden: list, out_dim=1, first_layer_float=True, bn_mode=\"fold\"):\n",
    "    assert bn_mode in {\"fold\",\"affine\",\"all\"}\n",
    "    dims = [in_dim] + list(hidden) + [out_dim]\n",
    "    weight_bytes = 0\n",
    "    for i in range(len(dims)-1):\n",
    "        fan_in, fan_out = dims[i], dims[i+1]\n",
    "        if i==0 and first_layer_float:\n",
    "            weight_bytes += fan_in*fan_out*4\n",
    "        else:\n",
    "            weight_bytes += ((fan_in*fan_out) + 7)//8\n",
    "    bias_bytes = 4  # output bias only in this architecture\n",
    "    if bn_mode==\"fold\":\n",
    "        bn_bytes = 0\n",
    "    else:\n",
    "        bn_channels = hidden[0] + sum(hidden[1:])\n",
    "        bn_bytes = (2 if bn_mode==\"affine\" else 4)*bn_channels*4\n",
    "    total = weight_bytes + bias_bytes + bn_bytes\n",
    "    return {\n",
    "        \"weight_kib\": round(weight_bytes/1024,4),\n",
    "        \"bias_kib\": round(bias_bytes/1024,4),\n",
    "        \"bn_kib\": round(bn_bytes/1024,4),\n",
    "        \"total_kib\": round(total/1024,4),\n",
    "    }\n",
    "\n",
    "def print_loaded_model_size(first_layer_float=True, bn_mode=\"fold\", binarize_mode=\"legacy\"):\n",
    "    m, preds, *_ = _get_model(binarize_mode)\n",
    "    sizes = param_size_kib(len(preds), m._inferred_hidden, 1, first_layer_float, bn_mode)\n",
    "    print(f\"Estimated deployment size (binarize={binarize_mode}, BN={bn_mode}):\")\n",
    "    for k,v in sizes.items(): print(f\"  {k}: {v} KiB\")\n",
    "    return sizes\n",
    "\n",
    "print_loaded_model_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e6c9a",
   "metadata": {},
   "source": [
    "## 10) Quick Usage Examples (unchanged paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df5aff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch probs: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Batch labels: [0 0 0 0 0 0 0 0]\n",
      "Batch logits: [-10.0155 -10.0155 -10.0155 -10.0155 -10.0155 -10.0155 -10.0155 -10.0155]\n",
      "Asthma prob (obvious case): 0.9982050657272339 Label: 1\n",
      "Logit: 6.320966720581055\n"
     ]
    }
   ],
   "source": [
    "# Batch check\n",
    "probs_b, labels_b, logits_b = predict_asthma(pd.read_csv('asthma_disease_data.csv').head(8))\n",
    "print('Batch probs:', probs_b.round(4))\n",
    "print('Batch labels:', labels_b)\n",
    "print('Batch logits:', logits_b.round(4))\n",
    "\n",
    "# Strong positive (obvious asthma) example row\n",
    "row_asthma = {\n",
    "    'Age': 25,\n",
    "    'Gender': 1,\n",
    "    'BMI': 39.29764739,\n",
    "    'Smoking': 0,\n",
    "    'PhysicalActivity': 8.899044846,\n",
    "    'DietQuality': 0.325397968,\n",
    "    'SleepQuality': 5.524751815,\n",
    "    'PollutionExposure': 7.854229872,\n",
    "    'PollenExposure': 0.498309572,\n",
    "    'DustExposure': 5.133637227,\n",
    "    'PetAllergy': 0,\n",
    "    'FamilyHistoryAsthma': 1,\n",
    "    'HistoryOfAllergies': 0,\n",
    "    'Eczema': 0,\n",
    "    'HayFever': 1,\n",
    "    'GastroesophagealReflux': 0,\n",
    "    'Wheezing': 1,\n",
    "    'ShortnessOfBreath': 0,\n",
    "    'ChestTightness': 0,\n",
    "    'Coughing': 0,\n",
    "    'NighttimeSymptoms': 0,\n",
    "    'ExerciseInduced': 1\n",
    "}\n",
    "\n",
    "p_asthma, l_asthma, logits = predict_asthma(row_asthma, threshold=0.5)\n",
    "print('Asthma prob (obvious case):', float(p_asthma[0]), 'Label:', int(l_asthma[0]))\n",
    "print('Logit:', float(logits[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
