{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asthma BNN Classifier (Balanced, ≤ 20KB params)\n",
    "\n",
    "This notebook trains a compact **Binarized Neural Network (BNN)** to predict asthma.\n",
    "\n",
    "**What this version does**\n",
    "- Drops **`Ethnicity`** from predictors\n",
    "- Uses **unweighted BCE** (no `pos_weight`)\n",
    "- **Oversamples positives** on the train split (avoid collapse without changing batch sizes)\n",
    "- **Hidden sizes = [64, 32]** to keep total params well **below 20KB** (ESP32-friendly)\n",
    "- Inference **matches training**: `fc → BN → hardtanh → dropout` (dropout disabled at eval)\n",
    "- Saves artifacts to **`/mnt/data/bnn_artifacts`** (unchanged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4536, 29)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Setup ===\n",
    "import os, json, math, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "CSV_PATH = Path('asthma_disease_data.csv')  # (unchanged path)\n",
    "assert CSV_PATH.exists(), f'CSV not found: {CSV_PATH}'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Clean & Feature Selection (drop `Ethnicity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns (examples): ['DoctorInCharge', 'EducationLevel', 'Ethnicity', 'LungFunctionFEV1', 'LungFunctionFVC', 'PatientID'] ...\n",
      "Predictors (22): ['Age', 'Gender', 'BMI', 'Smoking', 'PhysicalActivity', 'DietQuality', 'SleepQuality', 'PollutionExposure', 'PollenExposure', 'DustExposure', 'PetAllergy', 'FamilyHistoryAsthma', 'HistoryOfAllergies', 'Eczema', 'HayFever', 'GastroesophagealReflux', 'Wheezing', 'ShortnessOfBreath', 'ChestTightness', 'Coughing', 'NighttimeSymptoms', 'ExerciseInduced']\n",
      "Target: Diagnosis\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>DietQuality</th>\n",
       "      <th>SleepQuality</th>\n",
       "      <th>PollutionExposure</th>\n",
       "      <th>PollenExposure</th>\n",
       "      <th>DustExposure</th>\n",
       "      <th>...</th>\n",
       "      <th>Eczema</th>\n",
       "      <th>HayFever</th>\n",
       "      <th>GastroesophagealReflux</th>\n",
       "      <th>Wheezing</th>\n",
       "      <th>ShortnessOfBreath</th>\n",
       "      <th>ChestTightness</th>\n",
       "      <th>Coughing</th>\n",
       "      <th>NighttimeSymptoms</th>\n",
       "      <th>ExerciseInduced</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>15.848744</td>\n",
       "      <td>0</td>\n",
       "      <td>0.894448</td>\n",
       "      <td>5.488696</td>\n",
       "      <td>8.701003</td>\n",
       "      <td>7.388481</td>\n",
       "      <td>2.855578</td>\n",
       "      <td>0.974339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>22.757042</td>\n",
       "      <td>0</td>\n",
       "      <td>5.897329</td>\n",
       "      <td>6.341014</td>\n",
       "      <td>5.153966</td>\n",
       "      <td>1.969838</td>\n",
       "      <td>7.457665</td>\n",
       "      <td>6.584631</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>18.395396</td>\n",
       "      <td>0</td>\n",
       "      <td>6.739367</td>\n",
       "      <td>9.196237</td>\n",
       "      <td>6.840647</td>\n",
       "      <td>1.460593</td>\n",
       "      <td>1.448189</td>\n",
       "      <td>5.445799</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender        BMI  Smoking  PhysicalActivity  DietQuality  \\\n",
       "0   63       0  15.848744        0          0.894448     5.488696   \n",
       "1   26       1  22.757042        0          5.897329     6.341014   \n",
       "2   57       0  18.395396        0          6.739367     9.196237   \n",
       "\n",
       "   SleepQuality  PollutionExposure  PollenExposure  DustExposure  ...  Eczema  \\\n",
       "0      8.701003           7.388481        2.855578      0.974339  ...       0   \n",
       "1      5.153966           1.969838        7.457665      6.584631  ...       0   \n",
       "2      6.840647           1.460593        1.448189      5.445799  ...       0   \n",
       "\n",
       "   HayFever  GastroesophagealReflux  Wheezing  ShortnessOfBreath  \\\n",
       "0         0                       0         0                  0   \n",
       "1         0                       0         1                  0   \n",
       "2         1                       0         1                  1   \n",
       "\n",
       "   ChestTightness  Coughing  NighttimeSymptoms  ExerciseInduced  Diagnosis  \n",
       "0               1         0                  0                1          0  \n",
       "1               0         1                  1                1          0  \n",
       "2               1         0                  1                1          0  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_CANDIDATES = ['Diagnosis','asthma','Asthma','is_asthmatic']\n",
    "target_col = next((c for c in TARGET_CANDIDATES if c in df.columns), None)\n",
    "assert target_col is not None, f'Target column not found among {TARGET_CANDIDATES}'\n",
    "\n",
    "# Drop obvious identifiers and high-cardinality free text\n",
    "drop_cols = set()\n",
    "for c in df.columns:\n",
    "    if c == target_col:\n",
    "        continue\n",
    "    if c.lower() in ['patientid','doctorincharge','id','uuid','guid','recordid','ethnicity','educationlevel','lungfunctionfev1', 'lungfunctionfvc',]:\n",
    "        drop_cols.add(c)\n",
    "    if df[c].dtype == 'object' and df[c].nunique(dropna=True) > 20:\n",
    "        drop_cols.add(c)\n",
    "\n",
    "# Remove all-null, near-constant, duplicates\n",
    "for c in df.columns:\n",
    "    if c == target_col or c in drop_cols:\n",
    "        continue\n",
    "    s = df[c]\n",
    "    if s.isna().all():\n",
    "        drop_cols.add(c)\n",
    "    else:\n",
    "        top_freq = s.value_counts(dropna=False).iloc[0] / len(s)\n",
    "        if top_freq > 0.95:\n",
    "            drop_cols.add(c)\n",
    "\n",
    "seen = {}\n",
    "for c in df.columns:\n",
    "    if c == target_col or c in drop_cols:\n",
    "        continue\n",
    "    key = tuple(pd.util.hash_pandas_object(df[c].fillna('__NA__')).values)\n",
    "    if key in seen:\n",
    "        drop_cols.add(c)\n",
    "    else:\n",
    "        seen[key] = c\n",
    "\n",
    "df_clean = df.drop(columns=list(drop_cols), errors='ignore').copy()\n",
    "\n",
    "# Keep only numeric predictors\n",
    "predictors = [c for c in df_clean.columns if c != target_col and pd.api.types.is_numeric_dtype(df_clean[c])]\n",
    "df_clean = df_clean[predictors + [target_col]]\n",
    "\n",
    "# --- Enforce dropping Ethnicity ---\n",
    "if 'Ethnicity' in df_clean.columns:\n",
    "    df_clean = df_clean.drop(columns=['Ethnicity'])\n",
    "predictors = [c for c in df_clean.columns if c != target_col]\n",
    "\n",
    "print('Dropped columns (examples):', sorted(list(drop_cols))[:8], '...')\n",
    "print('Predictors ({}):'.format(len(predictors)), predictors)\n",
    "print('Target:', target_col)\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Split & Scale (unchanged batch sizes later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test: (3628, 22) (908, 22)\n",
      "Base positive rate (train): 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df_clean[target_col].astype(int).values\n",
    "assert set(np.unique(y)).issubset({0,1}), 'Target must be binary 0/1.'\n",
    "X = df_clean[predictors].astype(np.float32).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "x_mean = scaler.mean_.astype(np.float32)\n",
    "x_scale= scaler.scale_.astype(np.float32)\n",
    "\n",
    "print('Train/Test:', X_train.shape, X_test.shape)\n",
    "print('Base positive rate (train):', float((y_train==1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Oversample Positives (no pos_weight; no path/batch-size changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {0: 1814, 1: 1814}\n",
      "After : {0: 1814, 1: 1814}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_bal, y_train_bal = X_train, y_train\n",
    "print('Before:', dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print('After :', dict(zip(*np.unique(y_train_bal, return_counts=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) BNN Model (hidden=[64,32], param ≤ 20KB, exact forward at train & infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# ---------- binarizers ----------\n",
    "class SignBinarizeZeroToPlusOne(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x): return torch.where(x >= 0, torch.ones_like(x), -torch.ones_like(x))\n",
    "    @staticmethod\n",
    "    def backward(ctx, g): return g.clamp_(-1, 1)\n",
    "\n",
    "class SignBinarizeLegacy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x): return x.sign()   # -1, 0, +1 (0 stays 0)\n",
    "    @staticmethod\n",
    "    def backward(ctx, g): return g.clamp_(-1, 1)\n",
    "\n",
    "def make_binarize(mode: str):\n",
    "    mode = str(mode).lower()\n",
    "    assert mode in {\"legacy\",\"z2p1\"}\n",
    "    return SignBinarizeLegacy.apply if mode==\"legacy\" else SignBinarizeZeroToPlusOne.apply\n",
    "\n",
    "# ---------- model ----------\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=False, binarize_fn=None):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        nn.init.kaiming_normal_(self.weight, nonlinearity='relu')\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "        self.binarize = binarize_fn or make_binarize(\"legacy\")\n",
    "    def forward(self, x):\n",
    "        x_b = self.binarize(x)\n",
    "        alpha = self.weight.detach().abs().mean(dim=1, keepdim=True)\n",
    "        w_b = self.binarize(self.weight) * alpha\n",
    "        return F.linear(x_b, w_b, self.bias)\n",
    "\n",
    "class HybridBNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=[64,32], p_drop=0.1, binarize_mode=\"legacy\"):\n",
    "        super().__init__()\n",
    "        self.binarize = make_binarize(binarize_mode)\n",
    "        self.float1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden[0], bias=False),\n",
    "            nn.BatchNorm1d(hidden[0]),\n",
    "            nn.Hardtanh()\n",
    "        )\n",
    "        blocks = []\n",
    "        for i in range(len(hidden)-1):\n",
    "            blocks += [\n",
    "                BinaryLinear(hidden[i], hidden[i+1], bias=False, binarize_fn=self.binarize),\n",
    "                nn.BatchNorm1d(hidden[i+1]),\n",
    "                nn.Hardtanh(),\n",
    "                nn.Dropout(p_drop),\n",
    "            ]\n",
    "        self.bin_stack = nn.Sequential(*blocks) if blocks else nn.Identity()\n",
    "        self.out = BinaryLinear(hidden[-1], 1, bias=True, binarize_fn=self.binarize)\n",
    "    def forward(self, x):\n",
    "        x = self.float1(x)\n",
    "        x = self.bin_stack(x)\n",
    "        return self.out(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train (unweighted BCE; batch sizes unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | val_loss=0.7211 acc=0.550 f1=0.585 auc=0.573\n",
      "Epoch 05 | val_loss=0.6084 acc=0.664 f1=0.639 auc=0.734\n",
      "Epoch 10 | val_loss=0.5590 acc=0.704 f1=0.701 auc=0.787\n",
      "Epoch 15 | val_loss=0.5146 acc=0.765 f1=0.769 auc=0.826\n",
      "Epoch 20 | val_loss=0.4280 acc=0.816 f1=0.828 auc=0.890\n",
      "Epoch 25 | val_loss=0.4038 acc=0.827 f1=0.837 auc=0.901\n",
      "Epoch 30 | val_loss=0.3291 acc=0.889 f1=0.896 auc=0.934\n",
      "Epoch 35 | val_loss=0.3101 acc=0.887 f1=0.893 auc=0.943\n",
      "Epoch 40 | val_loss=0.2445 acc=0.901 f1=0.905 auc=0.964\n",
      "Epoch 45 | val_loss=0.2611 acc=0.894 f1=0.900 auc=0.963\n",
      "Epoch 50 | val_loss=0.2180 acc=0.922 f1=0.924 auc=0.968\n",
      "Epoch 55 | val_loss=0.1905 acc=0.946 f1=0.948 auc=0.975\n",
      "Epoch 60 | val_loss=0.2038 acc=0.934 f1=0.937 auc=0.973\n",
      "Epoch 65 | val_loss=0.1906 acc=0.937 f1=0.939 auc=0.975\n",
      "Epoch 70 | val_loss=0.2213 acc=0.927 f1=0.929 auc=0.968\n",
      "Epoch 75 | val_loss=0.2102 acc=0.936 f1=0.939 auc=0.970\n",
      "Epoch 80 | val_loss=0.2155 acc=0.943 f1=0.945 auc=0.972\n",
      "Epoch 85 | val_loss=0.1740 acc=0.955 f1=0.957 auc=0.981\n",
      "Epoch 90 | val_loss=0.1652 acc=0.956 f1=0.957 auc=0.981\n",
      "Epoch 95 | val_loss=0.1646 acc=0.956 f1=0.958 auc=0.984\n",
      "Epoch 100 | val_loss=0.1617 acc=0.960 f1=0.962 auc=0.983\n",
      "Epoch 105 | val_loss=0.1510 acc=0.963 f1=0.964 auc=0.987\n",
      "Epoch 110 | val_loss=0.1437 acc=0.959 f1=0.961 auc=0.985\n",
      "Epoch 115 | val_loss=0.1710 acc=0.955 f1=0.957 auc=0.987\n",
      "Epoch 120 | val_loss=0.1493 acc=0.964 f1=0.965 auc=0.985\n",
      "Epoch 125 | val_loss=0.1656 acc=0.958 f1=0.959 auc=0.986\n",
      "Epoch 130 | val_loss=0.1579 acc=0.963 f1=0.964 auc=0.988\n",
      "Epoch 135 | val_loss=0.1429 acc=0.963 f1=0.964 auc=0.993\n",
      "Epoch 140 | val_loss=0.1405 acc=0.967 f1=0.968 auc=0.991\n",
      "Epoch 145 | val_loss=0.1204 acc=0.960 f1=0.962 auc=0.996\n",
      "Epoch 150 | val_loss=0.1278 acc=0.963 f1=0.963 auc=0.990\n",
      "Epoch 155 | val_loss=0.1403 acc=0.965 f1=0.966 auc=0.991\n",
      "Epoch 160 | val_loss=0.1608 acc=0.959 f1=0.961 auc=0.993\n",
      "Epoch 165 | val_loss=0.1346 acc=0.968 f1=0.969 auc=0.993\n",
      "Epoch 170 | val_loss=0.1336 acc=0.959 f1=0.960 auc=0.992\n",
      "Epoch 175 | val_loss=0.1441 acc=0.965 f1=0.966 auc=0.993\n",
      "Epoch 180 | val_loss=0.1374 acc=0.965 f1=0.966 auc=0.991\n",
      "Epoch 185 | val_loss=0.1332 acc=0.970 f1=0.971 auc=0.994\n",
      "Epoch 190 | val_loss=0.1488 acc=0.969 f1=0.970 auc=0.992\n",
      "Epoch 195 | val_loss=0.1591 acc=0.964 f1=0.965 auc=0.992\n",
      "Epoch 200 | val_loss=0.1144 acc=0.966 f1=0.967 auc=0.997\n",
      "Epoch 205 | val_loss=0.1298 acc=0.975 f1=0.975 auc=0.993\n",
      "Epoch 210 | val_loss=0.1468 acc=0.970 f1=0.971 auc=0.990\n",
      "Epoch 215 | val_loss=0.1328 acc=0.976 f1=0.976 auc=0.994\n",
      "Epoch 220 | val_loss=0.1333 acc=0.978 f1=0.978 auc=0.990\n",
      "Epoch 225 | val_loss=0.1141 acc=0.976 f1=0.976 auc=0.996\n",
      "Epoch 230 | val_loss=0.1324 acc=0.975 f1=0.975 auc=0.993\n",
      "Epoch 235 | val_loss=0.1486 acc=0.975 f1=0.975 auc=0.992\n",
      "Epoch 240 | val_loss=0.1478 acc=0.967 f1=0.968 auc=0.995\n",
      "Epoch 245 | val_loss=0.1800 acc=0.967 f1=0.968 auc=0.992\n",
      "Epoch 250 | val_loss=0.1620 acc=0.969 f1=0.970 auc=0.992\n",
      "Epoch 255 | val_loss=0.1137 acc=0.976 f1=0.976 auc=0.997\n",
      "Epoch 260 | val_loss=0.1295 acc=0.974 f1=0.974 auc=0.998\n",
      "Epoch 265 | val_loss=0.0960 acc=0.974 f1=0.974 auc=0.996\n",
      "Epoch 270 | val_loss=0.1136 acc=0.972 f1=0.973 auc=0.997\n",
      "Epoch 275 | val_loss=0.0943 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 280 | val_loss=0.1525 acc=0.968 f1=0.969 auc=0.993\n",
      "Epoch 285 | val_loss=0.1557 acc=0.971 f1=0.972 auc=0.995\n",
      "Epoch 290 | val_loss=0.0979 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 295 | val_loss=0.1127 acc=0.972 f1=0.973 auc=0.996\n",
      "Epoch 300 | val_loss=0.1286 acc=0.970 f1=0.971 auc=0.996\n",
      "Epoch 305 | val_loss=0.1093 acc=0.977 f1=0.977 auc=0.996\n",
      "Epoch 310 | val_loss=0.1443 acc=0.969 f1=0.970 auc=0.997\n",
      "Epoch 315 | val_loss=0.1148 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 320 | val_loss=0.0723 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 325 | val_loss=0.1081 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 330 | val_loss=0.1028 acc=0.982 f1=0.983 auc=0.995\n",
      "Epoch 335 | val_loss=0.1272 acc=0.977 f1=0.977 auc=0.996\n",
      "Epoch 340 | val_loss=0.1068 acc=0.978 f1=0.978 auc=0.996\n",
      "Epoch 345 | val_loss=0.1493 acc=0.971 f1=0.972 auc=0.997\n",
      "Epoch 350 | val_loss=0.1023 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 355 | val_loss=0.1022 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 360 | val_loss=0.0953 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 365 | val_loss=0.0864 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 370 | val_loss=0.0954 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 375 | val_loss=0.1394 acc=0.972 f1=0.973 auc=0.997\n",
      "Epoch 380 | val_loss=0.1044 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 385 | val_loss=0.1045 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 390 | val_loss=0.0893 acc=0.979 f1=0.979 auc=0.998\n",
      "Epoch 395 | val_loss=0.1321 acc=0.981 f1=0.982 auc=0.995\n",
      "Epoch 400 | val_loss=0.1108 acc=0.980 f1=0.981 auc=0.996\n",
      "Epoch 405 | val_loss=0.0873 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 410 | val_loss=0.0933 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 415 | val_loss=0.1099 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 420 | val_loss=0.0919 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 425 | val_loss=0.1143 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 430 | val_loss=0.1174 acc=0.975 f1=0.975 auc=0.999\n",
      "Epoch 435 | val_loss=0.0754 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 440 | val_loss=0.0951 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 445 | val_loss=0.1114 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 450 | val_loss=0.0992 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 455 | val_loss=0.0877 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 460 | val_loss=0.1271 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 465 | val_loss=0.0928 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 470 | val_loss=0.1020 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 475 | val_loss=0.1341 acc=0.972 f1=0.973 auc=0.997\n",
      "Epoch 480 | val_loss=0.1263 acc=0.974 f1=0.974 auc=0.996\n",
      "Epoch 485 | val_loss=0.1036 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 490 | val_loss=0.0923 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 495 | val_loss=0.0737 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 500 | val_loss=0.0931 acc=0.983 f1=0.984 auc=0.997\n",
      "Epoch 505 | val_loss=0.0924 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 510 | val_loss=0.1332 acc=0.972 f1=0.973 auc=0.999\n",
      "Epoch 515 | val_loss=0.1020 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 520 | val_loss=0.0735 acc=0.985 f1=0.985 auc=0.997\n",
      "Epoch 525 | val_loss=0.0674 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 530 | val_loss=0.1112 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 535 | val_loss=0.1313 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 540 | val_loss=0.1182 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 545 | val_loss=0.0956 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 550 | val_loss=0.1364 acc=0.977 f1=0.977 auc=0.997\n",
      "Epoch 555 | val_loss=0.1269 acc=0.980 f1=0.981 auc=0.996\n",
      "Epoch 560 | val_loss=0.0954 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 565 | val_loss=0.1127 acc=0.985 f1=0.985 auc=0.996\n",
      "Epoch 570 | val_loss=0.1030 acc=0.987 f1=0.987 auc=0.995\n",
      "Epoch 575 | val_loss=0.1208 acc=0.986 f1=0.986 auc=0.997\n",
      "Epoch 580 | val_loss=0.1193 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 585 | val_loss=0.1082 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 590 | val_loss=0.1328 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 595 | val_loss=0.1650 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 600 | val_loss=0.1324 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 605 | val_loss=0.1276 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 610 | val_loss=0.1413 acc=0.974 f1=0.974 auc=0.999\n",
      "Epoch 615 | val_loss=0.1089 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 620 | val_loss=0.0763 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 625 | val_loss=0.1238 acc=0.980 f1=0.981 auc=0.995\n",
      "Epoch 630 | val_loss=0.1001 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 635 | val_loss=0.1103 acc=0.977 f1=0.977 auc=0.997\n",
      "Epoch 640 | val_loss=0.1280 acc=0.978 f1=0.978 auc=0.996\n",
      "Epoch 645 | val_loss=0.1762 acc=0.972 f1=0.973 auc=0.991\n",
      "Epoch 650 | val_loss=0.1247 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 655 | val_loss=0.1578 acc=0.972 f1=0.973 auc=0.998\n",
      "Epoch 660 | val_loss=0.0928 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 665 | val_loss=0.1201 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 670 | val_loss=0.1276 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 675 | val_loss=0.1231 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 680 | val_loss=0.1260 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 685 | val_loss=0.1242 acc=0.974 f1=0.974 auc=0.999\n",
      "Epoch 690 | val_loss=0.1132 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 695 | val_loss=0.1103 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 700 | val_loss=0.1121 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 705 | val_loss=0.1282 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 710 | val_loss=0.1213 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 715 | val_loss=0.1328 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 720 | val_loss=0.0926 acc=0.986 f1=0.986 auc=0.997\n",
      "Epoch 725 | val_loss=0.1016 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 730 | val_loss=0.1224 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 735 | val_loss=0.1039 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 740 | val_loss=0.1164 acc=0.976 f1=0.976 auc=0.996\n",
      "Epoch 745 | val_loss=0.0569 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 750 | val_loss=0.1223 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 755 | val_loss=0.0597 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 760 | val_loss=0.0865 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 765 | val_loss=0.1199 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 770 | val_loss=0.0712 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 775 | val_loss=0.0545 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 780 | val_loss=0.0883 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 785 | val_loss=0.0794 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 790 | val_loss=0.0505 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 795 | val_loss=0.0714 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 800 | val_loss=0.0898 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 805 | val_loss=0.1030 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 810 | val_loss=0.0741 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 815 | val_loss=0.0928 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 820 | val_loss=0.0808 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 825 | val_loss=0.1090 acc=0.975 f1=0.975 auc=1.000\n",
      "Epoch 830 | val_loss=0.0939 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 835 | val_loss=0.0510 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 840 | val_loss=0.0963 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 845 | val_loss=0.1236 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 850 | val_loss=0.1058 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 855 | val_loss=0.0964 acc=0.976 f1=0.976 auc=1.000\n",
      "Epoch 860 | val_loss=0.0780 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 865 | val_loss=0.1095 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 870 | val_loss=0.1494 acc=0.974 f1=0.974 auc=0.998\n",
      "Epoch 875 | val_loss=0.0921 acc=0.987 f1=0.987 auc=0.997\n",
      "Epoch 880 | val_loss=0.1002 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 885 | val_loss=0.0767 acc=0.979 f1=0.980 auc=1.000\n",
      "Epoch 890 | val_loss=0.1028 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 895 | val_loss=0.1023 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 900 | val_loss=0.1004 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 905 | val_loss=0.0792 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 910 | val_loss=0.1098 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 915 | val_loss=0.1145 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 920 | val_loss=0.0906 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 925 | val_loss=0.1325 acc=0.974 f1=0.974 auc=0.997\n",
      "Epoch 930 | val_loss=0.1406 acc=0.977 f1=0.977 auc=0.997\n",
      "Epoch 935 | val_loss=0.1070 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 940 | val_loss=0.0681 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 945 | val_loss=0.0832 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 950 | val_loss=0.1156 acc=0.976 f1=0.976 auc=0.997\n",
      "Epoch 955 | val_loss=0.1302 acc=0.976 f1=0.976 auc=0.997\n",
      "Epoch 960 | val_loss=0.1048 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 965 | val_loss=0.1015 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 970 | val_loss=0.1400 acc=0.976 f1=0.976 auc=0.997\n",
      "Epoch 975 | val_loss=0.1269 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 980 | val_loss=0.1436 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 985 | val_loss=0.0792 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 990 | val_loss=0.1191 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 995 | val_loss=0.1012 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1000 | val_loss=0.0784 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1005 | val_loss=0.1452 acc=0.976 f1=0.976 auc=0.996\n",
      "Epoch 1010 | val_loss=0.1612 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 1015 | val_loss=0.1182 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1020 | val_loss=0.0968 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 1025 | val_loss=0.1080 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1030 | val_loss=0.1164 acc=0.980 f1=0.981 auc=0.996\n",
      "Epoch 1035 | val_loss=0.1178 acc=0.980 f1=0.981 auc=0.996\n",
      "Epoch 1040 | val_loss=0.1345 acc=0.976 f1=0.976 auc=0.997\n",
      "Epoch 1045 | val_loss=0.1544 acc=0.974 f1=0.974 auc=0.995\n",
      "Epoch 1050 | val_loss=0.1333 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 1055 | val_loss=0.1139 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1060 | val_loss=0.1334 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 1065 | val_loss=0.0967 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1070 | val_loss=0.1187 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1075 | val_loss=0.1383 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 1080 | val_loss=0.1017 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1085 | val_loss=0.0887 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1090 | val_loss=0.1188 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1095 | val_loss=0.1177 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1100 | val_loss=0.1277 acc=0.974 f1=0.974 auc=0.997\n",
      "Epoch 1105 | val_loss=0.0808 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1110 | val_loss=0.0934 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1115 | val_loss=0.1260 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 1120 | val_loss=0.1006 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 1125 | val_loss=0.1142 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 1130 | val_loss=0.1200 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 1135 | val_loss=0.1254 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1140 | val_loss=0.1142 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 1145 | val_loss=0.1337 acc=0.977 f1=0.977 auc=0.996\n",
      "Epoch 1150 | val_loss=0.1226 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1155 | val_loss=0.1216 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1160 | val_loss=0.1354 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 1165 | val_loss=0.1081 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1170 | val_loss=0.1081 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1175 | val_loss=0.1443 acc=0.974 f1=0.974 auc=0.998\n",
      "Epoch 1180 | val_loss=0.1110 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1185 | val_loss=0.1110 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 1190 | val_loss=0.1259 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1195 | val_loss=0.0985 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1200 | val_loss=0.0928 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1205 | val_loss=0.0980 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1210 | val_loss=0.1157 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1215 | val_loss=0.1351 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 1220 | val_loss=0.1573 acc=0.976 f1=0.976 auc=0.996\n",
      "Epoch 1225 | val_loss=0.1689 acc=0.972 f1=0.973 auc=0.998\n",
      "Epoch 1230 | val_loss=0.1309 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1235 | val_loss=0.0840 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1240 | val_loss=0.1426 acc=0.975 f1=0.975 auc=0.996\n",
      "Epoch 1245 | val_loss=0.0787 acc=0.986 f1=0.986 auc=0.997\n",
      "Epoch 1250 | val_loss=0.1231 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1255 | val_loss=0.1034 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1260 | val_loss=0.1101 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1265 | val_loss=0.0958 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1270 | val_loss=0.0801 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1275 | val_loss=0.1275 acc=0.978 f1=0.978 auc=0.996\n",
      "Epoch 1280 | val_loss=0.1126 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 1285 | val_loss=0.1270 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 1290 | val_loss=0.0984 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1295 | val_loss=0.0780 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1300 | val_loss=0.0787 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1305 | val_loss=0.0976 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1310 | val_loss=0.0715 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1315 | val_loss=0.1039 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1320 | val_loss=0.1405 acc=0.978 f1=0.978 auc=0.995\n",
      "Epoch 1325 | val_loss=0.1095 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1330 | val_loss=0.1363 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 1335 | val_loss=0.1159 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1340 | val_loss=0.1440 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 1345 | val_loss=0.1157 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1350 | val_loss=0.1191 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1355 | val_loss=0.1280 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1360 | val_loss=0.1325 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1365 | val_loss=0.1084 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1370 | val_loss=0.0986 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1375 | val_loss=0.0899 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 1380 | val_loss=0.0688 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1385 | val_loss=0.0941 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1390 | val_loss=0.1005 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1395 | val_loss=0.1071 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1400 | val_loss=0.1258 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 1405 | val_loss=0.1205 acc=0.980 f1=0.981 auc=0.995\n",
      "Epoch 1410 | val_loss=0.1155 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 1415 | val_loss=0.1513 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 1420 | val_loss=0.1142 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1425 | val_loss=0.1099 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1430 | val_loss=0.1025 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1435 | val_loss=0.1131 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1440 | val_loss=0.1156 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1445 | val_loss=0.0990 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1450 | val_loss=0.1174 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1455 | val_loss=0.1075 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1460 | val_loss=0.1181 acc=0.977 f1=0.977 auc=1.000\n",
      "Epoch 1465 | val_loss=0.1319 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 1470 | val_loss=0.0971 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1475 | val_loss=0.1418 acc=0.976 f1=0.976 auc=1.000\n",
      "Epoch 1480 | val_loss=0.1300 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 1485 | val_loss=0.0964 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1490 | val_loss=0.0949 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1495 | val_loss=0.0714 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1500 | val_loss=0.1003 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1505 | val_loss=0.1161 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 1510 | val_loss=0.1089 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1515 | val_loss=0.1537 acc=0.969 f1=0.970 auc=1.000\n",
      "Epoch 1520 | val_loss=0.0967 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 1525 | val_loss=0.1066 acc=0.979 f1=0.980 auc=1.000\n",
      "Epoch 1530 | val_loss=0.0740 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 1535 | val_loss=0.0829 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 1540 | val_loss=0.0991 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1545 | val_loss=0.1091 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1550 | val_loss=0.0943 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1555 | val_loss=0.0934 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 1560 | val_loss=0.0552 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1565 | val_loss=0.0708 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1570 | val_loss=0.1120 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1575 | val_loss=0.0991 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1580 | val_loss=0.0949 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1585 | val_loss=0.0897 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1590 | val_loss=0.1093 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1595 | val_loss=0.0832 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1600 | val_loss=0.0833 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 1605 | val_loss=0.1126 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1610 | val_loss=0.0909 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1615 | val_loss=0.1065 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 1620 | val_loss=0.1057 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1625 | val_loss=0.0931 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1630 | val_loss=0.0636 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1635 | val_loss=0.0983 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1640 | val_loss=0.0806 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1645 | val_loss=0.0853 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1650 | val_loss=0.0969 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1655 | val_loss=0.1108 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1660 | val_loss=0.0917 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1665 | val_loss=0.0891 acc=0.985 f1=0.985 auc=0.997\n",
      "Epoch 1670 | val_loss=0.0883 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1675 | val_loss=0.0922 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1680 | val_loss=0.0961 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1685 | val_loss=0.0845 acc=0.980 f1=0.981 auc=1.000\n",
      "Epoch 1690 | val_loss=0.1010 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 1695 | val_loss=0.1105 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 1700 | val_loss=0.0959 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 1705 | val_loss=0.0962 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1710 | val_loss=0.1224 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 1715 | val_loss=0.1009 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1720 | val_loss=0.1301 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1725 | val_loss=0.1268 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1730 | val_loss=0.1248 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 1735 | val_loss=0.1525 acc=0.972 f1=0.973 auc=0.999\n",
      "Epoch 1740 | val_loss=0.1139 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1745 | val_loss=0.1060 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1750 | val_loss=0.1287 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1755 | val_loss=0.1378 acc=0.982 f1=0.983 auc=0.994\n",
      "Epoch 1760 | val_loss=0.1537 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1765 | val_loss=0.1110 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1770 | val_loss=0.1397 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1775 | val_loss=0.1123 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1780 | val_loss=0.1079 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1785 | val_loss=0.0967 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1790 | val_loss=0.0968 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1795 | val_loss=0.1027 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1800 | val_loss=0.0912 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1805 | val_loss=0.1081 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1810 | val_loss=0.1017 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1815 | val_loss=0.0938 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 1820 | val_loss=0.0890 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1825 | val_loss=0.1077 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1830 | val_loss=0.1116 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1835 | val_loss=0.1194 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1840 | val_loss=0.0963 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1845 | val_loss=0.1518 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 1850 | val_loss=0.0899 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1855 | val_loss=0.0960 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1860 | val_loss=0.1070 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1865 | val_loss=0.1311 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1870 | val_loss=0.1117 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1875 | val_loss=0.1274 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1880 | val_loss=0.1214 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1885 | val_loss=0.1174 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 1890 | val_loss=0.1043 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1895 | val_loss=0.0907 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1900 | val_loss=0.0956 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1905 | val_loss=0.0887 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1910 | val_loss=0.0985 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1915 | val_loss=0.0968 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1920 | val_loss=0.0879 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1925 | val_loss=0.0964 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1930 | val_loss=0.1141 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1935 | val_loss=0.1167 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1940 | val_loss=0.1424 acc=0.977 f1=0.977 auc=0.997\n",
      "Epoch 1945 | val_loss=0.1049 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1950 | val_loss=0.1118 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1955 | val_loss=0.1087 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1960 | val_loss=0.1521 acc=0.975 f1=0.975 auc=0.999\n",
      "Epoch 1965 | val_loss=0.1432 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 1970 | val_loss=0.1257 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 1975 | val_loss=0.1437 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 1980 | val_loss=0.0943 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1985 | val_loss=0.1337 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1990 | val_loss=0.1254 acc=0.977 f1=0.977 auc=0.995\n",
      "Epoch 1995 | val_loss=0.0952 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 2000 | val_loss=0.1262 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 2005 | val_loss=0.1346 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 2010 | val_loss=0.1247 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 2015 | val_loss=0.0893 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2020 | val_loss=0.1112 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 2025 | val_loss=0.1206 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 2030 | val_loss=0.1049 acc=0.983 f1=0.984 auc=0.997\n",
      "Epoch 2035 | val_loss=0.1184 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 2040 | val_loss=0.0842 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 2045 | val_loss=0.1112 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2050 | val_loss=0.0726 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2055 | val_loss=0.0996 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2060 | val_loss=0.0723 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2065 | val_loss=0.1081 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 2070 | val_loss=0.0853 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 2075 | val_loss=0.0784 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 2080 | val_loss=0.1093 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 2085 | val_loss=0.1057 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 2090 | val_loss=0.1076 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2095 | val_loss=0.1158 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 2100 | val_loss=0.1190 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 2105 | val_loss=0.1078 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 2110 | val_loss=0.1176 acc=0.982 f1=0.983 auc=0.995\n",
      "Epoch 2115 | val_loss=0.1355 acc=0.980 f1=0.981 auc=0.996\n",
      "Epoch 2120 | val_loss=0.1256 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 2125 | val_loss=0.0733 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2130 | val_loss=0.1048 acc=0.982 f1=0.983 auc=0.996\n",
      "Epoch 2135 | val_loss=0.1095 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 2140 | val_loss=0.1017 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 2145 | val_loss=0.1007 acc=0.987 f1=0.987 auc=0.996\n",
      "Epoch 2150 | val_loss=0.1011 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 2155 | val_loss=0.1088 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2160 | val_loss=0.0877 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2165 | val_loss=0.1024 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2170 | val_loss=0.0817 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2175 | val_loss=0.0849 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 2180 | val_loss=0.0713 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2185 | val_loss=0.0681 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2190 | val_loss=0.0737 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2195 | val_loss=0.0839 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 2200 | val_loss=0.0635 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2205 | val_loss=0.0858 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 2210 | val_loss=0.0491 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2215 | val_loss=0.0613 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2220 | val_loss=0.0753 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2225 | val_loss=0.0676 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2230 | val_loss=0.0817 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2235 | val_loss=0.0851 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2240 | val_loss=0.0677 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2245 | val_loss=0.0989 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 2250 | val_loss=0.0811 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2255 | val_loss=0.1363 acc=0.976 f1=0.976 auc=1.000\n",
      "Epoch 2260 | val_loss=0.1239 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 2265 | val_loss=0.1207 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 2270 | val_loss=0.0862 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2275 | val_loss=0.0954 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2280 | val_loss=0.0615 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2285 | val_loss=0.0994 acc=0.980 f1=0.981 auc=1.000\n",
      "Epoch 2290 | val_loss=0.0892 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2295 | val_loss=0.0722 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2300 | val_loss=0.0613 acc=0.989 f1=0.989 auc=0.998\n",
      "Epoch 2305 | val_loss=0.0775 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 2310 | val_loss=0.1099 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 2315 | val_loss=0.1013 acc=0.988 f1=0.988 auc=0.998\n",
      "Epoch 2320 | val_loss=0.0839 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2325 | val_loss=0.1058 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 2330 | val_loss=0.0557 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2335 | val_loss=0.0859 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2340 | val_loss=0.0704 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 2345 | val_loss=0.0967 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 2350 | val_loss=0.0881 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 2355 | val_loss=0.0863 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2360 | val_loss=0.0794 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2365 | val_loss=0.0725 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 2370 | val_loss=0.0911 acc=0.979 f1=0.980 auc=1.000\n",
      "Epoch 2375 | val_loss=0.1020 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 2380 | val_loss=0.0758 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2385 | val_loss=0.0738 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2390 | val_loss=0.0636 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2395 | val_loss=0.0517 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2400 | val_loss=0.1439 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 2405 | val_loss=0.1337 acc=0.975 f1=0.975 auc=0.999\n",
      "Epoch 2410 | val_loss=0.0740 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 2415 | val_loss=0.0870 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 2420 | val_loss=0.1260 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 2425 | val_loss=0.0991 acc=0.986 f1=0.986 auc=0.997\n",
      "Epoch 2430 | val_loss=0.0798 acc=0.986 f1=0.986 auc=0.997\n",
      "Epoch 2435 | val_loss=0.1155 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 2440 | val_loss=0.0585 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2445 | val_loss=0.1137 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 2450 | val_loss=0.0789 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2455 | val_loss=0.1239 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 2460 | val_loss=0.1409 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 2465 | val_loss=0.0861 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2470 | val_loss=0.0953 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 2475 | val_loss=0.0987 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 2480 | val_loss=0.1142 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2485 | val_loss=0.1140 acc=0.986 f1=0.986 auc=0.996\n",
      "Epoch 2490 | val_loss=0.1406 acc=0.980 f1=0.981 auc=0.996\n",
      "Epoch 2495 | val_loss=0.1013 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2500 | val_loss=0.0928 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 2505 | val_loss=0.1130 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 2510 | val_loss=0.1154 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 2515 | val_loss=0.1069 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 2520 | val_loss=0.1179 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2525 | val_loss=0.0874 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 2530 | val_loss=0.1092 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 2535 | val_loss=0.0995 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 2540 | val_loss=0.1171 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 2545 | val_loss=0.0958 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2550 | val_loss=0.1280 acc=0.980 f1=0.981 auc=1.000\n",
      "Epoch 2555 | val_loss=0.1145 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 2560 | val_loss=0.0762 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 2565 | val_loss=0.1168 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 2570 | val_loss=0.0848 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2575 | val_loss=0.1102 acc=0.979 f1=0.980 auc=1.000\n",
      "Epoch 2580 | val_loss=0.1107 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 2585 | val_loss=0.1035 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2590 | val_loss=0.1196 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2595 | val_loss=0.1060 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 2600 | val_loss=0.0846 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2605 | val_loss=0.1361 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 2610 | val_loss=0.1063 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 2615 | val_loss=0.0877 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2620 | val_loss=0.0968 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2625 | val_loss=0.0581 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2630 | val_loss=0.0596 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2635 | val_loss=0.0701 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2640 | val_loss=0.0721 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2645 | val_loss=0.0879 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2650 | val_loss=0.0896 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 2655 | val_loss=0.1019 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2660 | val_loss=0.0899 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2665 | val_loss=0.0721 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2670 | val_loss=0.0796 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2675 | val_loss=0.0780 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2680 | val_loss=0.0590 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2685 | val_loss=0.0600 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2690 | val_loss=0.0659 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2695 | val_loss=0.0906 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2700 | val_loss=0.1066 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2705 | val_loss=0.1022 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2710 | val_loss=0.0814 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2715 | val_loss=0.0589 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2720 | val_loss=0.0476 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2725 | val_loss=0.0672 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2730 | val_loss=0.0738 acc=0.991 f1=0.991 auc=0.999\n",
      "Epoch 2735 | val_loss=0.0711 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2740 | val_loss=0.0915 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2745 | val_loss=0.0694 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2750 | val_loss=0.1106 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2755 | val_loss=0.0685 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2760 | val_loss=0.0941 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2765 | val_loss=0.0663 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2770 | val_loss=0.0915 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2775 | val_loss=0.0804 acc=0.988 f1=0.988 auc=0.997\n",
      "Epoch 2780 | val_loss=0.0610 acc=0.990 f1=0.990 auc=0.998\n",
      "Epoch 2785 | val_loss=0.0589 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2790 | val_loss=0.0587 acc=0.991 f1=0.991 auc=0.999\n",
      "Epoch 2795 | val_loss=0.0918 acc=0.989 f1=0.989 auc=0.998\n",
      "Epoch 2800 | val_loss=0.0556 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 2805 | val_loss=0.0563 acc=0.992 f1=0.992 auc=0.999\n",
      "Epoch 2810 | val_loss=0.0598 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 2815 | val_loss=0.0797 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2820 | val_loss=0.0740 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2825 | val_loss=0.0627 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2830 | val_loss=0.0940 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2835 | val_loss=0.0745 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2840 | val_loss=0.0689 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2845 | val_loss=0.0574 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2850 | val_loss=0.0587 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2855 | val_loss=0.0715 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2860 | val_loss=0.0444 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2865 | val_loss=0.0509 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2870 | val_loss=0.0635 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2875 | val_loss=0.0618 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2880 | val_loss=0.0496 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2885 | val_loss=0.0969 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2890 | val_loss=0.1221 acc=0.980 f1=0.981 auc=1.000\n",
      "Epoch 2895 | val_loss=0.1244 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 2900 | val_loss=0.0852 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2905 | val_loss=0.0726 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2910 | val_loss=0.0754 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2915 | val_loss=0.0590 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2920 | val_loss=0.0464 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2925 | val_loss=0.0661 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2930 | val_loss=0.0714 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2935 | val_loss=0.0959 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2940 | val_loss=0.0963 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 2945 | val_loss=0.0579 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2950 | val_loss=0.0566 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2955 | val_loss=0.0734 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2960 | val_loss=0.0938 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2965 | val_loss=0.0893 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2970 | val_loss=0.0872 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2975 | val_loss=0.0916 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2980 | val_loss=0.0793 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2985 | val_loss=0.0703 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2990 | val_loss=0.0415 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2995 | val_loss=0.0415 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 3000 | val_loss=0.0394 acc=0.991 f1=0.991 auc=1.000\n",
      "Best @ epoch 2859 val_acc 0.992\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# DataLoaders (batch sizes unchanged)\n",
    "Xtr = torch.from_numpy(X_train_bal).to(device)\n",
    "ytr = torch.from_numpy(y_train_bal.astype(np.float32)).to(device)\n",
    "Xte = torch.from_numpy(X_test).to(device)\n",
    "yte = torch.from_numpy(y_test.astype(np.float32)).to(device)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr,ytr), batch_size=256, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(Xte,yte), batch_size=512, shuffle=False)\n",
    "\n",
    "model = HybridBNN(in_dim=X_train.shape[1]).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # unweighted loss\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, n = 0.0, 0\n",
    "    all_logits, all_y = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            total_loss += float(loss.item()) * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_y.append(yb.detach().cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    y_true = torch.cat(all_y).numpy()\n",
    "    probs = 1/(1+np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    return total_loss/max(1,n), acc, f1, auc\n",
    "\n",
    "best = {\"epoch\":-1, \"loss\":1e9, \"acc\":0.0, \"state\":None}\n",
    "EPOCHS = 3000\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "    val_loss, acc, f1, auc = evaluate()\n",
    "    if val_loss < best[\"loss\"]:\n",
    "        best.update({\"epoch\":epoch, \"loss\":val_loss, \"acc\":acc, \"state\":model.state_dict()})\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:02d} | val_loss={val_loss:.4f} acc={acc:.3f} f1={f1:.3f} auc={auc:.3f}\")\n",
    "\n",
    "model.load_state_dict(best[\"state\"])\n",
    "print(\"Best @ epoch\", best[\"epoch\"], \"val_acc\", round(best[\"acc\"],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Evaluation + Threshold Sweep (find a useful operating point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9824    0.9911       454\n",
      "           1     0.9827    1.0000    0.9913       454\n",
      "\n",
      "    accuracy                         0.9912       908\n",
      "   macro avg     0.9913    0.9912    0.9912       908\n",
      "weighted avg     0.9913    0.9912    0.9912       908\n",
      "\n",
      "ROC-AUC: 1.0\n",
      "PR-AUC : 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.9994687438011169 Best F1: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000       454\n",
      "           1     1.0000    1.0000    1.0000       454\n",
      "\n",
      "    accuracy                         1.0000       908\n",
      "   macro avg     1.0000    1.0000    1.0000       908\n",
      "weighted avg     1.0000    1.0000    1.0000       908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.from_numpy(X_test).to(device)).cpu().numpy()\n",
    "probs = 1/(1+np.exp(-logits))\n",
    "preds = (probs>=0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, preds, digits=4))\n",
    "try:\n",
    "    print('ROC-AUC:', roc_auc_score(y_test, probs))\n",
    "    print('PR-AUC :', average_precision_score(y_test, probs))\n",
    "except Exception as e:\n",
    "    print('AUC error:', e)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(); plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "plt.title('Confusion Matrix'); plt.colorbar();\n",
    "plt.xticks([0,1], ['No Asthma','Asthma']); plt.yticks([0,1], ['No Asthma','Asthma'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Threshold sweep (F1)\n",
    "prec, rec, thr = precision_recall_curve(y_test, probs)\n",
    "best_t, best_f1 = 0.5, -1.0\n",
    "for t in np.unique(np.clip(thr, 0, 1)):\n",
    "    pred = (probs >= t).astype(int)\n",
    "    f1 = f1_score(y_test, pred, zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_t = f1, float(t)\n",
    "print('Best threshold:', best_t, 'Best F1:', round(best_f1,4))\n",
    "print(classification_report(y_test, (probs>=best_t).astype(int), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Save Artifacts (unchanged path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to bnn_artifacts\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path('bnn_artifacts')  # unchanged\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "torch.save(model.state_dict(), out_dir/'bnn_state.pt')\n",
    "with open(out_dir/'scaler.json','w') as f:\n",
    "    json.dump({'mean': x_mean.tolist(), 'scale': x_scale.tolist(), 'predictors': predictors, 'target': target_col}, f)\n",
    "print('Saved to', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Robust Prediction Helper (exact-forward; path auto-discovery inc. `/mnt/data/bnn_artifacts`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CANDIDATE_DIRS = ['bnn_artifacts','./bnn_artifacts','/mnt/data/bnn_artifacts', str(Path.cwd()/'bnn_artifacts')]\n",
    "_model_cache = {\"legacy\": None, \"z2p1\": None}\n",
    "\n",
    "def _find_art_dir():\n",
    "    for p in _CANDIDATE_DIRS:\n",
    "        d = Path(p)\n",
    "        if (d/'bnn_state.pt').exists() and (d/'scaler.json').exists():\n",
    "            return d\n",
    "    raise FileNotFoundError('Artifacts not found in: ' + ', '.join(_CANDIDATE_DIRS))\n",
    "\n",
    "def _infer_hidden_from_state(state, input_dim):\n",
    "    w2d = [(k,v) for k,v in state.items() if k.endswith('.weight') and hasattr(v,'ndim') and v.ndim==2]\n",
    "    hidden, first = [], False\n",
    "    for k,W in w2d:\n",
    "        out_f, in_f = W.shape\n",
    "        if not first:\n",
    "            if in_f != input_dim: continue\n",
    "            hidden.append(out_f); first = True; continue\n",
    "        if out_f == 1: break\n",
    "        hidden.append(out_f)\n",
    "    if not hidden: raise ValueError(\"Failed to infer hidden sizes from checkpoint.\")\n",
    "    return hidden\n",
    "\n",
    "def _build_bnn_from_state(state, input_dim, p_drop=0.1, binarize_mode=\"legacy\"):\n",
    "    hidden = _infer_hidden_from_state(state, input_dim)\n",
    "    m = HybridBNN(input_dim, hidden, p_drop=p_drop, binarize_mode=binarize_mode)\n",
    "    m._inferred_hidden = hidden\n",
    "    return m\n",
    "\n",
    "def _load_artifacts_for_infer(binarize_mode=\"legacy\"):\n",
    "    art = _find_art_dir()\n",
    "    meta = json.loads((art/'scaler.json').read_text())\n",
    "    preds  = meta['predictors']\n",
    "    x_mean = np.array(meta['mean'],  dtype=np.float32)\n",
    "    x_scale= np.array(meta['scale'], dtype=np.float32)\n",
    "    # honor metadata hint if present\n",
    "    bm = str(meta.get('binarize_mode', binarize_mode)).lower()\n",
    "    if bm in {\"legacy\",\"z2p1\"}: binarize_mode = bm\n",
    "    state = torch.load(art/'bnn_state.pt', map_location='cpu')\n",
    "    m = _build_bnn_from_state(state, input_dim=len(preds), p_drop=0.1, binarize_mode=binarize_mode)\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    m.eval()\n",
    "    print(f\"✅ Loaded from {art} | hidden sizes: {m._inferred_hidden} | binarize={binarize_mode}\")\n",
    "    return m, preds, x_mean, x_scale\n",
    "\n",
    "def _get_model(binarize_mode=\"legacy\"):\n",
    "    if _model_cache[binarize_mode] is None:\n",
    "        _model_cache[binarize_mode] = _load_artifacts_for_infer(binarize_mode)\n",
    "    return _model_cache[binarize_mode]\n",
    "\n",
    "def reset_infer_cache():\n",
    "    for k in _model_cache: _model_cache[k] = None\n",
    "\n",
    "# ---------- preprocessing & prediction ----------\n",
    "def _prep_df(df_like, predictors, x_mean, x_scale, strict=False):\n",
    "    if isinstance(df_like, dict):\n",
    "        df = pd.DataFrame([df_like])\n",
    "    elif isinstance(df_like, pd.Series):\n",
    "        df = pd.DataFrame([df_like.to_dict()])\n",
    "    else:\n",
    "        df = df_like.copy()\n",
    "\n",
    "    # enforce exact predictor order; fill missing with training mean (warn)\n",
    "    missing = [c for c in predictors if c not in df.columns]\n",
    "    if missing:\n",
    "        if strict:\n",
    "            raise KeyError(f\"Missing predictors: {missing}\")\n",
    "        print(f\"[WARN] Filling {len(missing)} missing column(s) with training mean: {missing[:8]}{'...' if len(missing)>8 else ''}\")\n",
    "        for c, mu in zip(predictors, x_mean):\n",
    "            if c not in df.columns:\n",
    "                df[c] = float(mu)\n",
    "\n",
    "    X = df[predictors].astype(np.float32).values\n",
    "    Xn = (X - x_mean) / (x_scale + 1e-8)\n",
    "    return Xn\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_asthma(df_like, threshold: float = 0.5, binarize_mode=\"legacy\", strict=False, verbose=False):\n",
    "    m, preds, mu, sigma = _get_model(binarize_mode)\n",
    "    Xn = _prep_df(df_like, preds, mu, sigma, strict=strict)\n",
    "    lg = m(torch.from_numpy(Xn)).cpu().numpy()\n",
    "    pr = 1/(1+np.exp(-lg))\n",
    "    lb = (pr >= float(threshold)).astype(np.int32)\n",
    "    if verbose:\n",
    "        print(f\"[{binarize_mode}] logits: min={lg.min():.4f} max={lg.max():.4f} mean={lg.mean():.4f}\")\n",
    "        print(f\"[{binarize_mode}] probs : min={pr.min():.4f} max={pr.max():.4f} mean={pr.mean():.4f}\")\n",
    "    return pr, lb, lg\n",
    "\n",
    "def compare_binarize_modes(df_like, threshold=0.5, strict=False):\n",
    "    out = {}\n",
    "    for mode in [\"legacy\",\"z2p1\"]:\n",
    "        try:\n",
    "            out[mode] = predict_asthma(df_like, threshold, binarize_mode=mode, strict=strict, verbose=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[{mode}] ERROR:\", e); out[mode]=None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Parameter Size (KiB) — confirm ≤ 20KB total (fold BN for deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- size estimator ----------\n",
    "def param_size_kib(in_dim: int, hidden: list, out_dim=1, first_layer_float=True, bn_mode=\"fold\"):\n",
    "    assert bn_mode in {\"fold\",\"affine\",\"all\"}\n",
    "    dims = [in_dim] + list(hidden) + [out_dim]\n",
    "    weight_bytes = 0\n",
    "    for i in range(len(dims)-1):\n",
    "        fan_in, fan_out = dims[i], dims[i+1]\n",
    "        if i==0 and first_layer_float:\n",
    "            weight_bytes += fan_in*fan_out*4\n",
    "        else:\n",
    "            weight_bytes += ((fan_in*fan_out) + 7)//8\n",
    "    bias_bytes = 4  # output bias only in this architecture\n",
    "    if bn_mode==\"fold\":\n",
    "        bn_bytes = 0\n",
    "    else:\n",
    "        bn_channels = hidden[0] + sum(hidden[1:])\n",
    "        bn_bytes = (2 if bn_mode==\"affine\" else 4)*bn_channels*4\n",
    "    total = weight_bytes + bias_bytes + bn_bytes\n",
    "    return {\n",
    "        \"weight_kib\": round(weight_bytes/1024,4),\n",
    "        \"bias_kib\": round(bias_bytes/1024,4),\n",
    "        \"bn_kib\": round(bn_bytes/1024,4),\n",
    "        \"total_kib\": round(total/1024,4),\n",
    "    }\n",
    "\n",
    "def print_loaded_model_size(first_layer_float=True, bn_mode=\"fold\", binarize_mode=\"legacy\"):\n",
    "    m, preds, *_ = _get_model(binarize_mode)\n",
    "    sizes = param_size_kib(len(preds), m._inferred_hidden, 1, first_layer_float, bn_mode)\n",
    "    print(f\"Estimated deployment size (binarize={binarize_mode}, BN={bn_mode}):\")\n",
    "    for k,v in sizes.items(): print(f\"  {k}: {v} KiB\")\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Quick Usage Examples (unchanged paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded from bnn_artifacts | hidden sizes: [64, 32] | binarize=legacy\n",
      "Batch probs: [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Batch labels: [0 0 0 0 0 0 0 0]\n",
      "Batch logits: [-13.7174 -12.0169 -13.7174 -13.7174 -13.7174 -12.0169 -13.7174 -13.7174]\n",
      "Asthma prob (obvious case): 0.9999923706054688 Label: 1\n",
      "Logit: 11.791178703308105\n"
     ]
    }
   ],
   "source": [
    "# Batch check\n",
    "probs_b, labels_b, logits_b = predict_asthma(pd.read_csv('asthma_disease_data.csv').head(8))\n",
    "print('Batch probs:', probs_b.round(4))\n",
    "print('Batch labels:', labels_b)\n",
    "print('Batch logits:', logits_b.round(4))\n",
    "\n",
    "# Strong positive (obvious asthma) example row\n",
    "row_asthma = {\n",
    "    'Age': 25,\n",
    "    'Gender': 1,\n",
    "    'BMI': 39.29764739,\n",
    "    'Smoking': 0,\n",
    "    'PhysicalActivity': 8.899044846,\n",
    "    'DietQuality': 0.325397968,\n",
    "    'SleepQuality': 5.524751815,\n",
    "    'PollutionExposure': 7.854229872,\n",
    "    'PollenExposure': 0.498309572,\n",
    "    'DustExposure': 5.133637227,\n",
    "    'PetAllergy': 0,\n",
    "    'FamilyHistoryAsthma': 1,\n",
    "    'HistoryOfAllergies': 0,\n",
    "    'Eczema': 0,\n",
    "    'HayFever': 1,\n",
    "    'GastroesophagealReflux': 0,\n",
    "    'Wheezing': 1,\n",
    "    'ShortnessOfBreath': 0,\n",
    "    'ChestTightness': 0,\n",
    "    'Coughing': 0,\n",
    "    'NighttimeSymptoms': 0,\n",
    "    'ExerciseInduced': 1\n",
    "}\n",
    "\n",
    "p_asthma, l_asthma, logits = predict_asthma(row_asthma, threshold=0.5)\n",
    "print('Asthma prob (obvious case):', float(p_asthma[0]), 'Label:', int(l_asthma[0]))\n",
    "print('Logit:', float(logits[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
