{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asthma BNN Classifier (Balanced, ≤ 20KB params)\n",
    "\n",
    "This notebook trains a compact **Binarized Neural Network (BNN)** to predict asthma.\n",
    "\n",
    "**What this version does**\n",
    "- Drops **`Ethnicity`** from predictors\n",
    "- Uses **unweighted BCE** (no `pos_weight`)\n",
    "- **Oversamples positives** on the train split (avoid collapse without changing batch sizes)\n",
    "- **Hidden sizes = [64, 32]** to keep total params well **below 20KB** (ESP32-friendly)\n",
    "- Inference **matches training**: `fc → BN → hardtanh → dropout` (dropout disabled at eval)\n",
    "- Saves artifacts to **`/mnt/data/bnn_artifacts`** (unchanged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4536, 29)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Setup ===\n",
    "import os, json, math, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "CSV_PATH = Path('asthma_disease_data.csv')  # (unchanged path)\n",
    "assert CSV_PATH.exists(), f'CSV not found: {CSV_PATH}'\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Clean & Feature Selection (drop `Ethnicity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns (examples): ['DoctorInCharge', 'EducationLevel', 'Ethnicity', 'LungFunctionFEV1', 'LungFunctionFVC', 'PatientID'] ...\n",
      "Predictors (22): ['Age', 'Gender', 'BMI', 'Smoking', 'PhysicalActivity', 'DietQuality', 'SleepQuality', 'PollutionExposure', 'PollenExposure', 'DustExposure', 'PetAllergy', 'FamilyHistoryAsthma', 'HistoryOfAllergies', 'Eczema', 'HayFever', 'GastroesophagealReflux', 'Wheezing', 'ShortnessOfBreath', 'ChestTightness', 'Coughing', 'NighttimeSymptoms', 'ExerciseInduced']\n",
      "Target: Diagnosis\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>DietQuality</th>\n",
       "      <th>SleepQuality</th>\n",
       "      <th>PollutionExposure</th>\n",
       "      <th>PollenExposure</th>\n",
       "      <th>DustExposure</th>\n",
       "      <th>...</th>\n",
       "      <th>Eczema</th>\n",
       "      <th>HayFever</th>\n",
       "      <th>GastroesophagealReflux</th>\n",
       "      <th>Wheezing</th>\n",
       "      <th>ShortnessOfBreath</th>\n",
       "      <th>ChestTightness</th>\n",
       "      <th>Coughing</th>\n",
       "      <th>NighttimeSymptoms</th>\n",
       "      <th>ExerciseInduced</th>\n",
       "      <th>Diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>15.848744</td>\n",
       "      <td>0</td>\n",
       "      <td>0.894448</td>\n",
       "      <td>5.488696</td>\n",
       "      <td>8.701003</td>\n",
       "      <td>7.388481</td>\n",
       "      <td>2.855578</td>\n",
       "      <td>0.974339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>22.757042</td>\n",
       "      <td>0</td>\n",
       "      <td>5.897329</td>\n",
       "      <td>6.341014</td>\n",
       "      <td>5.153966</td>\n",
       "      <td>1.969838</td>\n",
       "      <td>7.457665</td>\n",
       "      <td>6.584631</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>18.395396</td>\n",
       "      <td>0</td>\n",
       "      <td>6.739367</td>\n",
       "      <td>9.196237</td>\n",
       "      <td>6.840647</td>\n",
       "      <td>1.460593</td>\n",
       "      <td>1.448189</td>\n",
       "      <td>5.445799</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender        BMI  Smoking  PhysicalActivity  DietQuality  \\\n",
       "0   63       0  15.848744        0          0.894448     5.488696   \n",
       "1   26       1  22.757042        0          5.897329     6.341014   \n",
       "2   57       0  18.395396        0          6.739367     9.196237   \n",
       "\n",
       "   SleepQuality  PollutionExposure  PollenExposure  DustExposure  ...  Eczema  \\\n",
       "0      8.701003           7.388481        2.855578      0.974339  ...       0   \n",
       "1      5.153966           1.969838        7.457665      6.584631  ...       0   \n",
       "2      6.840647           1.460593        1.448189      5.445799  ...       0   \n",
       "\n",
       "   HayFever  GastroesophagealReflux  Wheezing  ShortnessOfBreath  \\\n",
       "0         0                       0         0                  0   \n",
       "1         0                       0         1                  0   \n",
       "2         1                       0         1                  1   \n",
       "\n",
       "   ChestTightness  Coughing  NighttimeSymptoms  ExerciseInduced  Diagnosis  \n",
       "0               1         0                  0                1          0  \n",
       "1               0         1                  1                1          0  \n",
       "2               1         0                  1                1          0  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGET_CANDIDATES = ['Diagnosis','asthma','Asthma','is_asthmatic']\n",
    "target_col = next((c for c in TARGET_CANDIDATES if c in df.columns), None)\n",
    "assert target_col is not None, f'Target column not found among {TARGET_CANDIDATES}'\n",
    "\n",
    "# Drop obvious identifiers and high-cardinality free text\n",
    "drop_cols = set()\n",
    "for c in df.columns:\n",
    "    if c == target_col:\n",
    "        continue\n",
    "    if c.lower() in ['patientid','doctorincharge','id','uuid','guid','recordid','ethnicity','educationlevel','lungfunctionfev1', 'lungfunctionfvc',]:\n",
    "        drop_cols.add(c)\n",
    "    if df[c].dtype == 'object' and df[c].nunique(dropna=True) > 20:\n",
    "        drop_cols.add(c)\n",
    "\n",
    "# Remove all-null, near-constant, duplicates\n",
    "for c in df.columns:\n",
    "    if c == target_col or c in drop_cols:\n",
    "        continue\n",
    "    s = df[c]\n",
    "    if s.isna().all():\n",
    "        drop_cols.add(c)\n",
    "    else:\n",
    "        top_freq = s.value_counts(dropna=False).iloc[0] / len(s)\n",
    "        if top_freq > 0.95:\n",
    "            drop_cols.add(c)\n",
    "\n",
    "seen = {}\n",
    "for c in df.columns:\n",
    "    if c == target_col or c in drop_cols:\n",
    "        continue\n",
    "    key = tuple(pd.util.hash_pandas_object(df[c].fillna('__NA__')).values)\n",
    "    if key in seen:\n",
    "        drop_cols.add(c)\n",
    "    else:\n",
    "        seen[key] = c\n",
    "\n",
    "df_clean = df.drop(columns=list(drop_cols), errors='ignore').copy()\n",
    "\n",
    "# Keep only numeric predictors\n",
    "predictors = [c for c in df_clean.columns if c != target_col and pd.api.types.is_numeric_dtype(df_clean[c])]\n",
    "df_clean = df_clean[predictors + [target_col]]\n",
    "\n",
    "# --- Enforce dropping Ethnicity ---\n",
    "if 'Ethnicity' in df_clean.columns:\n",
    "    df_clean = df_clean.drop(columns=['Ethnicity'])\n",
    "predictors = [c for c in df_clean.columns if c != target_col]\n",
    "\n",
    "print('Dropped columns (examples):', sorted(list(drop_cols))[:8], '...')\n",
    "print('Predictors ({}):'.format(len(predictors)), predictors)\n",
    "print('Target:', target_col)\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Split & Scale (unchanged batch sizes later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test: (3628, 22) (908, 22)\n",
      "Base positive rate (train): 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df_clean[target_col].astype(int).values\n",
    "assert set(np.unique(y)).issubset({0,1}), 'Target must be binary 0/1.'\n",
    "X = df_clean[predictors].astype(np.float32).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "x_mean = scaler.mean_.astype(np.float32)\n",
    "x_scale= scaler.scale_.astype(np.float32)\n",
    "\n",
    "print('Train/Test:', X_train.shape, X_test.shape)\n",
    "print('Base positive rate (train):', float((y_train==1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Oversample Positives (no pos_weight; no path/batch-size changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {0: 1814, 1: 1814}\n",
      "After : {0: 1814, 1: 1814}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_bal, y_train_bal = X_train, y_train\n",
    "print('Before:', dict(zip(*np.unique(y_train, return_counts=True))))\n",
    "print('After :', dict(zip(*np.unique(y_train_bal, return_counts=True))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) BNN Model (hidden=[64,32], param ≤ 20KB, exact forward at train & infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# ---------- binarizers ----------\n",
    "class SignBinarizeZeroToPlusOne(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x): return torch.where(x >= 0, torch.ones_like(x), -torch.ones_like(x))\n",
    "    @staticmethod\n",
    "    def backward(ctx, g): return g.clamp_(-1, 1)\n",
    "\n",
    "class SignBinarizeLegacy(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x): return x.sign()   # -1, 0, +1 (0 stays 0)\n",
    "    @staticmethod\n",
    "    def backward(ctx, g): return g.clamp_(-1, 1)\n",
    "\n",
    "def make_binarize(mode: str):\n",
    "    mode = str(mode).lower()\n",
    "    assert mode in {\"legacy\",\"z2p1\"}\n",
    "    return SignBinarizeLegacy.apply if mode==\"legacy\" else SignBinarizeZeroToPlusOne.apply\n",
    "\n",
    "# ---------- model ----------\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=False, binarize_fn=None):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        nn.init.kaiming_normal_(self.weight, nonlinearity='relu')\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "        self.binarize = binarize_fn or make_binarize(\"legacy\")\n",
    "    def forward(self, x):\n",
    "        x_b = self.binarize(x)\n",
    "        alpha = self.weight.detach().abs().mean(dim=1, keepdim=True)\n",
    "        w_b = self.binarize(self.weight) * alpha\n",
    "        return F.linear(x_b, w_b, self.bias)\n",
    "\n",
    "class HybridBNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=[64,32], p_drop=0.1, binarize_mode=\"legacy\"):\n",
    "        super().__init__()\n",
    "        self.binarize = make_binarize(binarize_mode)\n",
    "        self.float1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden[0], bias=False),\n",
    "            nn.BatchNorm1d(hidden[0]),\n",
    "            nn.Hardtanh()\n",
    "        )\n",
    "        blocks = []\n",
    "        for i in range(len(hidden)-1):\n",
    "            blocks += [\n",
    "                BinaryLinear(hidden[i], hidden[i+1], bias=False, binarize_fn=self.binarize),\n",
    "                nn.BatchNorm1d(hidden[i+1]),\n",
    "                nn.Hardtanh(),\n",
    "                nn.Dropout(p_drop),\n",
    "            ]\n",
    "        self.bin_stack = nn.Sequential(*blocks) if blocks else nn.Identity()\n",
    "        self.out = BinaryLinear(hidden[-1], 1, bias=True, binarize_fn=self.binarize)\n",
    "    def forward(self, x):\n",
    "        x = self.float1(x)\n",
    "        x = self.bin_stack(x)\n",
    "        return self.out(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Train (unweighted BCE; batch sizes unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | val_loss=0.7318 acc=0.546 f1=0.516 auc=0.582\n",
      "Epoch 05 | val_loss=0.6269 acc=0.620 f1=0.566 auc=0.712\n",
      "Epoch 10 | val_loss=0.5459 acc=0.725 f1=0.715 auc=0.797\n",
      "Epoch 15 | val_loss=0.4765 acc=0.787 f1=0.797 auc=0.861\n",
      "Epoch 20 | val_loss=0.4388 acc=0.804 f1=0.804 auc=0.878\n",
      "Epoch 25 | val_loss=0.3618 acc=0.865 f1=0.870 auc=0.918\n",
      "Epoch 30 | val_loss=0.3127 acc=0.881 f1=0.885 auc=0.935\n",
      "Epoch 35 | val_loss=0.2630 acc=0.905 f1=0.910 auc=0.956\n",
      "Epoch 40 | val_loss=0.2487 acc=0.917 f1=0.921 auc=0.958\n",
      "Epoch 45 | val_loss=0.2276 acc=0.920 f1=0.925 auc=0.967\n",
      "Epoch 50 | val_loss=0.2591 acc=0.916 f1=0.922 auc=0.955\n",
      "Epoch 55 | val_loss=0.2188 acc=0.937 f1=0.941 auc=0.970\n",
      "Epoch 60 | val_loss=0.1946 acc=0.931 f1=0.933 auc=0.977\n",
      "Epoch 65 | val_loss=0.1930 acc=0.936 f1=0.939 auc=0.974\n",
      "Epoch 70 | val_loss=0.2177 acc=0.938 f1=0.941 auc=0.976\n",
      "Epoch 75 | val_loss=0.1751 acc=0.945 f1=0.948 auc=0.983\n",
      "Epoch 80 | val_loss=0.1587 acc=0.948 f1=0.950 auc=0.985\n",
      "Epoch 85 | val_loss=0.1516 acc=0.956 f1=0.958 auc=0.987\n",
      "Epoch 90 | val_loss=0.1596 acc=0.955 f1=0.957 auc=0.986\n",
      "Epoch 95 | val_loss=0.1556 acc=0.960 f1=0.962 auc=0.985\n",
      "Epoch 100 | val_loss=0.1702 acc=0.954 f1=0.955 auc=0.986\n",
      "Epoch 105 | val_loss=0.1291 acc=0.966 f1=0.967 auc=0.989\n",
      "Epoch 110 | val_loss=0.1391 acc=0.961 f1=0.963 auc=0.988\n",
      "Epoch 115 | val_loss=0.1151 acc=0.966 f1=0.967 auc=0.992\n",
      "Epoch 120 | val_loss=0.1023 acc=0.977 f1=0.977 auc=0.993\n",
      "Epoch 125 | val_loss=0.1065 acc=0.967 f1=0.968 auc=0.993\n",
      "Epoch 130 | val_loss=0.1302 acc=0.971 f1=0.972 auc=0.990\n",
      "Epoch 135 | val_loss=0.1133 acc=0.976 f1=0.976 auc=0.990\n",
      "Epoch 140 | val_loss=0.0850 acc=0.980 f1=0.981 auc=0.995\n",
      "Epoch 145 | val_loss=0.1656 acc=0.967 f1=0.968 auc=0.987\n",
      "Epoch 150 | val_loss=0.1283 acc=0.967 f1=0.968 auc=0.995\n",
      "Epoch 155 | val_loss=0.1332 acc=0.961 f1=0.963 auc=0.993\n",
      "Epoch 160 | val_loss=0.0970 acc=0.977 f1=0.977 auc=0.993\n",
      "Epoch 165 | val_loss=0.1138 acc=0.968 f1=0.969 auc=0.996\n",
      "Epoch 170 | val_loss=0.0910 acc=0.982 f1=0.983 auc=0.993\n",
      "Epoch 175 | val_loss=0.1089 acc=0.977 f1=0.977 auc=0.995\n",
      "Epoch 180 | val_loss=0.0878 acc=0.978 f1=0.978 auc=0.995\n",
      "Epoch 185 | val_loss=0.1072 acc=0.974 f1=0.974 auc=0.996\n",
      "Epoch 190 | val_loss=0.1073 acc=0.970 f1=0.971 auc=0.994\n",
      "Epoch 195 | val_loss=0.1238 acc=0.970 f1=0.971 auc=0.993\n",
      "Epoch 200 | val_loss=0.1123 acc=0.979 f1=0.980 auc=0.994\n",
      "Epoch 205 | val_loss=0.1492 acc=0.969 f1=0.970 auc=0.992\n",
      "Epoch 210 | val_loss=0.1420 acc=0.971 f1=0.972 auc=0.990\n",
      "Epoch 215 | val_loss=0.1414 acc=0.966 f1=0.967 auc=0.994\n",
      "Epoch 220 | val_loss=0.1210 acc=0.964 f1=0.965 auc=0.994\n",
      "Epoch 225 | val_loss=0.1307 acc=0.972 f1=0.973 auc=0.994\n",
      "Epoch 230 | val_loss=0.1321 acc=0.976 f1=0.976 auc=0.993\n",
      "Epoch 235 | val_loss=0.1254 acc=0.975 f1=0.975 auc=0.995\n",
      "Epoch 240 | val_loss=0.1579 acc=0.969 f1=0.970 auc=0.991\n",
      "Epoch 245 | val_loss=0.1405 acc=0.970 f1=0.971 auc=0.994\n",
      "Epoch 250 | val_loss=0.1443 acc=0.974 f1=0.974 auc=0.995\n",
      "Epoch 255 | val_loss=0.1340 acc=0.974 f1=0.974 auc=0.994\n",
      "Epoch 260 | val_loss=0.1522 acc=0.975 f1=0.975 auc=0.991\n",
      "Epoch 265 | val_loss=0.1498 acc=0.972 f1=0.973 auc=0.992\n",
      "Epoch 270 | val_loss=0.1475 acc=0.971 f1=0.972 auc=0.994\n",
      "Epoch 275 | val_loss=0.1783 acc=0.963 f1=0.964 auc=0.992\n",
      "Epoch 280 | val_loss=0.1323 acc=0.971 f1=0.972 auc=0.996\n",
      "Epoch 285 | val_loss=0.1208 acc=0.979 f1=0.980 auc=0.995\n",
      "Epoch 290 | val_loss=0.1756 acc=0.972 f1=0.973 auc=0.993\n",
      "Epoch 295 | val_loss=0.1464 acc=0.975 f1=0.975 auc=0.993\n",
      "Epoch 300 | val_loss=0.1072 acc=0.977 f1=0.977 auc=0.995\n",
      "Epoch 305 | val_loss=0.1150 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 310 | val_loss=0.1042 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 315 | val_loss=0.1070 acc=0.978 f1=0.978 auc=0.996\n",
      "Epoch 320 | val_loss=0.0930 acc=0.983 f1=0.984 auc=0.995\n",
      "Epoch 325 | val_loss=0.0992 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 330 | val_loss=0.1016 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 335 | val_loss=0.1082 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 340 | val_loss=0.1065 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 345 | val_loss=0.1285 acc=0.971 f1=0.972 auc=0.997\n",
      "Epoch 350 | val_loss=0.1188 acc=0.971 f1=0.972 auc=0.997\n",
      "Epoch 355 | val_loss=0.1233 acc=0.976 f1=0.976 auc=0.995\n",
      "Epoch 360 | val_loss=0.1027 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 365 | val_loss=0.0990 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 370 | val_loss=0.1072 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 375 | val_loss=0.0978 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 380 | val_loss=0.1142 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 385 | val_loss=0.0857 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 390 | val_loss=0.1124 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 395 | val_loss=0.0911 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 400 | val_loss=0.0919 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 405 | val_loss=0.1161 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 410 | val_loss=0.0882 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 415 | val_loss=0.1143 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 420 | val_loss=0.1228 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 425 | val_loss=0.0868 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 430 | val_loss=0.0612 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 435 | val_loss=0.1106 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 440 | val_loss=0.0923 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 445 | val_loss=0.0970 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 450 | val_loss=0.1051 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 455 | val_loss=0.0953 acc=0.983 f1=0.984 auc=0.995\n",
      "Epoch 460 | val_loss=0.0996 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 465 | val_loss=0.0674 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 470 | val_loss=0.1347 acc=0.977 f1=0.977 auc=0.996\n",
      "Epoch 475 | val_loss=0.1396 acc=0.975 f1=0.975 auc=0.995\n",
      "Epoch 480 | val_loss=0.1160 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 485 | val_loss=0.0921 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 490 | val_loss=0.0961 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 495 | val_loss=0.0719 acc=0.980 f1=0.980 auc=0.998\n",
      "Epoch 500 | val_loss=0.0941 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 505 | val_loss=0.0996 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 510 | val_loss=0.1146 acc=0.972 f1=0.973 auc=0.998\n",
      "Epoch 515 | val_loss=0.1151 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 520 | val_loss=0.1253 acc=0.974 f1=0.974 auc=0.999\n",
      "Epoch 525 | val_loss=0.0985 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 530 | val_loss=0.0798 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 535 | val_loss=0.1268 acc=0.972 f1=0.973 auc=0.998\n",
      "Epoch 540 | val_loss=0.1199 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 545 | val_loss=0.0908 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 550 | val_loss=0.1084 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 555 | val_loss=0.0785 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 560 | val_loss=0.0877 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 565 | val_loss=0.1124 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 570 | val_loss=0.0646 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 575 | val_loss=0.1031 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 580 | val_loss=0.0954 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 585 | val_loss=0.1006 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 590 | val_loss=0.1089 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 595 | val_loss=0.0881 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 600 | val_loss=0.1040 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 605 | val_loss=0.1143 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 610 | val_loss=0.0962 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 615 | val_loss=0.0966 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 620 | val_loss=0.0740 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 625 | val_loss=0.1102 acc=0.979 f1=0.980 auc=0.998\n",
      "Epoch 630 | val_loss=0.0872 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 635 | val_loss=0.0859 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 640 | val_loss=0.0763 acc=0.987 f1=0.987 auc=0.997\n",
      "Epoch 645 | val_loss=0.1062 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 650 | val_loss=0.1031 acc=0.974 f1=0.974 auc=1.000\n",
      "Epoch 655 | val_loss=0.0984 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 660 | val_loss=0.1111 acc=0.974 f1=0.974 auc=0.999\n",
      "Epoch 665 | val_loss=0.0846 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 670 | val_loss=0.0784 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 675 | val_loss=0.1192 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 680 | val_loss=0.1203 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 685 | val_loss=0.1071 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 690 | val_loss=0.0911 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 695 | val_loss=0.0661 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 700 | val_loss=0.0700 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 705 | val_loss=0.0564 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 710 | val_loss=0.1075 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 715 | val_loss=0.0851 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 720 | val_loss=0.1114 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 725 | val_loss=0.1342 acc=0.977 f1=0.977 auc=0.996\n",
      "Epoch 730 | val_loss=0.1255 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 735 | val_loss=0.0860 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 740 | val_loss=0.0853 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 745 | val_loss=0.1101 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 750 | val_loss=0.1290 acc=0.974 f1=0.974 auc=0.996\n",
      "Epoch 755 | val_loss=0.1385 acc=0.974 f1=0.974 auc=1.000\n",
      "Epoch 760 | val_loss=0.1662 acc=0.976 f1=0.976 auc=0.996\n",
      "Epoch 765 | val_loss=0.0904 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 770 | val_loss=0.1265 acc=0.977 f1=0.977 auc=0.998\n",
      "Epoch 775 | val_loss=0.0564 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 780 | val_loss=0.0618 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 785 | val_loss=0.0911 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 790 | val_loss=0.0791 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 795 | val_loss=0.0768 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 800 | val_loss=0.0587 acc=0.992 f1=0.992 auc=0.999\n",
      "Epoch 805 | val_loss=0.1387 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 810 | val_loss=0.0808 acc=0.987 f1=0.987 auc=0.997\n",
      "Epoch 815 | val_loss=0.1024 acc=0.983 f1=0.984 auc=0.996\n",
      "Epoch 820 | val_loss=0.1232 acc=0.981 f1=0.982 auc=0.996\n",
      "Epoch 825 | val_loss=0.1067 acc=0.978 f1=0.978 auc=0.997\n",
      "Epoch 830 | val_loss=0.0853 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 835 | val_loss=0.1014 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 840 | val_loss=0.1511 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 845 | val_loss=0.1333 acc=0.974 f1=0.974 auc=0.999\n",
      "Epoch 850 | val_loss=0.0926 acc=0.981 f1=0.982 auc=0.997\n",
      "Epoch 855 | val_loss=0.0970 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 860 | val_loss=0.1082 acc=0.979 f1=0.980 auc=0.996\n",
      "Epoch 865 | val_loss=0.1040 acc=0.982 f1=0.983 auc=0.996\n",
      "Epoch 870 | val_loss=0.0940 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 875 | val_loss=0.1289 acc=0.976 f1=0.976 auc=0.997\n",
      "Epoch 880 | val_loss=0.0934 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 885 | val_loss=0.1208 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 890 | val_loss=0.0675 acc=0.989 f1=0.989 auc=0.998\n",
      "Epoch 895 | val_loss=0.0707 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 900 | val_loss=0.1123 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 905 | val_loss=0.0835 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 910 | val_loss=0.0813 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 915 | val_loss=0.1084 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 920 | val_loss=0.0933 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 925 | val_loss=0.0641 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 930 | val_loss=0.0814 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 935 | val_loss=0.0728 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 940 | val_loss=0.0752 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 945 | val_loss=0.0938 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 950 | val_loss=0.0934 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 955 | val_loss=0.0823 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 960 | val_loss=0.0804 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 965 | val_loss=0.0745 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 970 | val_loss=0.1129 acc=0.985 f1=0.985 auc=0.997\n",
      "Epoch 975 | val_loss=0.0994 acc=0.983 f1=0.984 auc=0.997\n",
      "Epoch 980 | val_loss=0.0558 acc=0.988 f1=0.988 auc=0.998\n",
      "Epoch 985 | val_loss=0.0627 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 990 | val_loss=0.1210 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 995 | val_loss=0.0950 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 1000 | val_loss=0.1065 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1005 | val_loss=0.1175 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1010 | val_loss=0.1169 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1015 | val_loss=0.1058 acc=0.979 f1=0.980 auc=0.997\n",
      "Epoch 1020 | val_loss=0.1203 acc=0.981 f1=0.982 auc=0.995\n",
      "Epoch 1025 | val_loss=0.0987 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1030 | val_loss=0.1076 acc=0.983 f1=0.984 auc=0.997\n",
      "Epoch 1035 | val_loss=0.0769 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 1040 | val_loss=0.0818 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1045 | val_loss=0.0855 acc=0.989 f1=0.989 auc=0.997\n",
      "Epoch 1050 | val_loss=0.1139 acc=0.986 f1=0.986 auc=0.995\n",
      "Epoch 1055 | val_loss=0.1007 acc=0.987 f1=0.987 auc=0.995\n",
      "Epoch 1060 | val_loss=0.0886 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 1065 | val_loss=0.0836 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1070 | val_loss=0.0653 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1075 | val_loss=0.0833 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1080 | val_loss=0.1307 acc=0.974 f1=0.974 auc=0.997\n",
      "Epoch 1085 | val_loss=0.0886 acc=0.980 f1=0.981 auc=0.997\n",
      "Epoch 1090 | val_loss=0.0877 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1095 | val_loss=0.1137 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1100 | val_loss=0.1016 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1105 | val_loss=0.0985 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1110 | val_loss=0.0856 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1115 | val_loss=0.1073 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1120 | val_loss=0.0865 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 1125 | val_loss=0.0778 acc=0.990 f1=0.990 auc=0.998\n",
      "Epoch 1130 | val_loss=0.0933 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1135 | val_loss=0.0848 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1140 | val_loss=0.1085 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1145 | val_loss=0.0963 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1150 | val_loss=0.0987 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1155 | val_loss=0.1498 acc=0.978 f1=0.978 auc=0.994\n",
      "Epoch 1160 | val_loss=0.1704 acc=0.974 f1=0.974 auc=0.996\n",
      "Epoch 1165 | val_loss=0.1338 acc=0.975 f1=0.975 auc=0.999\n",
      "Epoch 1170 | val_loss=0.0719 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1175 | val_loss=0.0555 acc=0.991 f1=0.991 auc=0.999\n",
      "Epoch 1180 | val_loss=0.0640 acc=0.987 f1=0.987 auc=0.997\n",
      "Epoch 1185 | val_loss=0.0617 acc=0.988 f1=0.988 auc=0.998\n",
      "Epoch 1190 | val_loss=0.0390 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 1195 | val_loss=0.0571 acc=0.989 f1=0.989 auc=0.998\n",
      "Epoch 1200 | val_loss=0.0658 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 1205 | val_loss=0.0951 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 1210 | val_loss=0.0752 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 1215 | val_loss=0.0745 acc=0.988 f1=0.988 auc=0.998\n",
      "Epoch 1220 | val_loss=0.0721 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1225 | val_loss=0.0746 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 1230 | val_loss=0.0680 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 1235 | val_loss=0.0991 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 1240 | val_loss=0.0638 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 1245 | val_loss=0.0883 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1250 | val_loss=0.0661 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1255 | val_loss=0.0919 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1260 | val_loss=0.0966 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1265 | val_loss=0.0915 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1270 | val_loss=0.0932 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1275 | val_loss=0.1042 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1280 | val_loss=0.0997 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1285 | val_loss=0.0792 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 1290 | val_loss=0.0676 acc=0.989 f1=0.989 auc=0.998\n",
      "Epoch 1295 | val_loss=0.0897 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 1300 | val_loss=0.1044 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1305 | val_loss=0.0793 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1310 | val_loss=0.0582 acc=0.987 f1=0.987 auc=0.997\n",
      "Epoch 1315 | val_loss=0.0793 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1320 | val_loss=0.0689 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1325 | val_loss=0.0561 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1330 | val_loss=0.1257 acc=0.975 f1=0.975 auc=0.999\n",
      "Epoch 1335 | val_loss=0.1213 acc=0.974 f1=0.974 auc=0.999\n",
      "Epoch 1340 | val_loss=0.0970 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1345 | val_loss=0.0595 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1350 | val_loss=0.0726 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1355 | val_loss=0.0872 acc=0.985 f1=0.985 auc=0.997\n",
      "Epoch 1360 | val_loss=0.0942 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1365 | val_loss=0.0693 acc=0.991 f1=0.991 auc=0.998\n",
      "Epoch 1370 | val_loss=0.1137 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1375 | val_loss=0.0908 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1380 | val_loss=0.1235 acc=0.977 f1=0.977 auc=1.000\n",
      "Epoch 1385 | val_loss=0.0950 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1390 | val_loss=0.0768 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 1395 | val_loss=0.1016 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1400 | val_loss=0.1298 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 1405 | val_loss=0.1132 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1410 | val_loss=0.1205 acc=0.975 f1=0.975 auc=0.998\n",
      "Epoch 1415 | val_loss=0.1084 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1420 | val_loss=0.0954 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 1425 | val_loss=0.0782 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 1430 | val_loss=0.0965 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1435 | val_loss=0.1118 acc=0.978 f1=0.978 auc=0.998\n",
      "Epoch 1440 | val_loss=0.0878 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 1445 | val_loss=0.0552 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 1450 | val_loss=0.0812 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1455 | val_loss=0.0719 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1460 | val_loss=0.0709 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1465 | val_loss=0.0536 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1470 | val_loss=0.0661 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1475 | val_loss=0.0747 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 1480 | val_loss=0.0819 acc=0.986 f1=0.986 auc=0.998\n",
      "Epoch 1485 | val_loss=0.0732 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1490 | val_loss=0.0801 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1495 | val_loss=0.0712 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 1500 | val_loss=0.0891 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 1505 | val_loss=0.0569 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1510 | val_loss=0.0613 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1515 | val_loss=0.0712 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 1520 | val_loss=0.0690 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 1525 | val_loss=0.0778 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1530 | val_loss=0.0671 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 1535 | val_loss=0.0559 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1540 | val_loss=0.0724 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1545 | val_loss=0.0759 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1550 | val_loss=0.0837 acc=0.978 f1=0.978 auc=1.000\n",
      "Epoch 1555 | val_loss=0.0561 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1560 | val_loss=0.0633 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 1565 | val_loss=0.0567 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1570 | val_loss=0.0807 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 1575 | val_loss=0.0680 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1580 | val_loss=0.0562 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 1585 | val_loss=0.0859 acc=0.983 f1=0.984 auc=0.997\n",
      "Epoch 1590 | val_loss=0.0685 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 1595 | val_loss=0.0541 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1600 | val_loss=0.0372 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 1605 | val_loss=0.0435 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 1610 | val_loss=0.0238 acc=0.994 f1=0.995 auc=1.000\n",
      "Epoch 1615 | val_loss=0.0543 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1620 | val_loss=0.0674 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1625 | val_loss=0.0695 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1630 | val_loss=0.0930 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 1635 | val_loss=0.0627 acc=0.992 f1=0.992 auc=0.999\n",
      "Epoch 1640 | val_loss=0.0872 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 1645 | val_loss=0.0667 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1650 | val_loss=0.1177 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 1655 | val_loss=0.0492 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1660 | val_loss=0.0696 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 1665 | val_loss=0.0692 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1670 | val_loss=0.0363 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 1675 | val_loss=0.0660 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 1680 | val_loss=0.0483 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1685 | val_loss=0.0816 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 1690 | val_loss=0.0448 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1695 | val_loss=0.0600 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1700 | val_loss=0.0467 acc=0.992 f1=0.992 auc=0.999\n",
      "Epoch 1705 | val_loss=0.0439 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1710 | val_loss=0.0568 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 1715 | val_loss=0.0561 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1720 | val_loss=0.0498 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1725 | val_loss=0.0534 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 1730 | val_loss=0.0425 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 1735 | val_loss=0.0428 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 1740 | val_loss=0.0364 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 1745 | val_loss=0.0515 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1750 | val_loss=0.0397 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1755 | val_loss=0.0158 acc=0.994 f1=0.995 auc=1.000\n",
      "Epoch 1760 | val_loss=0.0392 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1765 | val_loss=0.0541 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1770 | val_loss=0.0539 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 1775 | val_loss=0.0264 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 1780 | val_loss=0.0423 acc=0.993 f1=0.993 auc=0.999\n",
      "Epoch 1785 | val_loss=0.0540 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 1790 | val_loss=0.0337 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 1795 | val_loss=0.0518 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 1800 | val_loss=0.0426 acc=0.991 f1=0.991 auc=0.999\n",
      "Epoch 1805 | val_loss=0.0386 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 1810 | val_loss=0.0561 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 1815 | val_loss=0.0690 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1820 | val_loss=0.0404 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 1825 | val_loss=0.0512 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 1830 | val_loss=0.0460 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 1835 | val_loss=0.0657 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1840 | val_loss=0.0734 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1845 | val_loss=0.0586 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1850 | val_loss=0.0527 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1855 | val_loss=0.0447 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 1860 | val_loss=0.1073 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1865 | val_loss=0.0777 acc=0.980 f1=0.981 auc=1.000\n",
      "Epoch 1870 | val_loss=0.0731 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1875 | val_loss=0.0953 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 1880 | val_loss=0.0897 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 1885 | val_loss=0.0795 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 1890 | val_loss=0.1137 acc=0.979 f1=0.980 auc=0.999\n",
      "Epoch 1895 | val_loss=0.1543 acc=0.975 f1=0.975 auc=0.997\n",
      "Epoch 1900 | val_loss=0.1179 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 1905 | val_loss=0.0977 acc=0.980 f1=0.981 auc=1.000\n",
      "Epoch 1910 | val_loss=0.0553 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1915 | val_loss=0.1200 acc=0.976 f1=0.976 auc=0.999\n",
      "Epoch 1920 | val_loss=0.1308 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 1925 | val_loss=0.1202 acc=0.975 f1=0.975 auc=0.999\n",
      "Epoch 1930 | val_loss=0.1221 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 1935 | val_loss=0.0996 acc=0.979 f1=0.980 auc=1.000\n",
      "Epoch 1940 | val_loss=0.1049 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1945 | val_loss=0.0882 acc=0.980 f1=0.981 auc=1.000\n",
      "Epoch 1950 | val_loss=0.1178 acc=0.976 f1=0.976 auc=0.998\n",
      "Epoch 1955 | val_loss=0.0812 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 1960 | val_loss=0.0872 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 1965 | val_loss=0.1078 acc=0.978 f1=0.978 auc=0.999\n",
      "Epoch 1970 | val_loss=0.0871 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 1975 | val_loss=0.1120 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 1980 | val_loss=0.1073 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 1985 | val_loss=0.0695 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1990 | val_loss=0.0809 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 1995 | val_loss=0.0987 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2000 | val_loss=0.1308 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 2005 | val_loss=0.0836 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2010 | val_loss=0.0712 acc=0.988 f1=0.988 auc=0.997\n",
      "Epoch 2015 | val_loss=0.1067 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 2020 | val_loss=0.1010 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2025 | val_loss=0.0897 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2030 | val_loss=0.0974 acc=0.983 f1=0.984 auc=0.998\n",
      "Epoch 2035 | val_loss=0.1014 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 2040 | val_loss=0.1039 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 2045 | val_loss=0.0855 acc=0.981 f1=0.982 auc=0.999\n",
      "Epoch 2050 | val_loss=0.0861 acc=0.985 f1=0.985 auc=0.998\n",
      "Epoch 2055 | val_loss=0.0706 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2060 | val_loss=0.0738 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2065 | val_loss=0.0753 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2070 | val_loss=0.0477 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2075 | val_loss=0.0659 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2080 | val_loss=0.0717 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2085 | val_loss=0.0418 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2090 | val_loss=0.0598 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2095 | val_loss=0.0709 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2100 | val_loss=0.0510 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2105 | val_loss=0.0533 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2110 | val_loss=0.0505 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2115 | val_loss=0.0543 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2120 | val_loss=0.0627 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2125 | val_loss=0.0551 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2130 | val_loss=0.0614 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2135 | val_loss=0.0622 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2140 | val_loss=0.0586 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2145 | val_loss=0.0706 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2150 | val_loss=0.0670 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2155 | val_loss=0.0959 acc=0.982 f1=0.983 auc=0.997\n",
      "Epoch 2160 | val_loss=0.0809 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 2165 | val_loss=0.1372 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 2170 | val_loss=0.0959 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2175 | val_loss=0.0509 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2180 | val_loss=0.0753 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2185 | val_loss=0.0383 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2190 | val_loss=0.0611 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2195 | val_loss=0.0304 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2200 | val_loss=0.0552 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2205 | val_loss=0.0598 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 2210 | val_loss=0.0479 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2215 | val_loss=0.0206 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 2220 | val_loss=0.0447 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2225 | val_loss=0.0749 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2230 | val_loss=0.0654 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2235 | val_loss=0.0663 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2240 | val_loss=0.0761 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2245 | val_loss=0.0620 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2250 | val_loss=0.0821 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 2255 | val_loss=0.0945 acc=0.981 f1=0.982 auc=1.000\n",
      "Epoch 2260 | val_loss=0.0618 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2265 | val_loss=0.0560 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2270 | val_loss=0.0330 acc=0.994 f1=0.995 auc=0.999\n",
      "Epoch 2275 | val_loss=0.0820 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2280 | val_loss=0.0709 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2285 | val_loss=0.0533 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2290 | val_loss=0.0481 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2295 | val_loss=0.0385 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2300 | val_loss=0.0416 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 2305 | val_loss=0.0361 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2310 | val_loss=0.0411 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2315 | val_loss=0.0337 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2320 | val_loss=0.0380 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2325 | val_loss=0.0451 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2330 | val_loss=0.0664 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2335 | val_loss=0.0894 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2340 | val_loss=0.0815 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2345 | val_loss=0.0667 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2350 | val_loss=0.0725 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2355 | val_loss=0.1150 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 2360 | val_loss=0.1021 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2365 | val_loss=0.0644 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2370 | val_loss=0.0771 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2375 | val_loss=0.0512 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 2380 | val_loss=0.0652 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2385 | val_loss=0.0841 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2390 | val_loss=0.0871 acc=0.979 f1=0.980 auc=1.000\n",
      "Epoch 2395 | val_loss=0.0774 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2400 | val_loss=0.0635 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2405 | val_loss=0.0646 acc=0.992 f1=0.992 auc=0.998\n",
      "Epoch 2410 | val_loss=0.0521 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2415 | val_loss=0.0327 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 2420 | val_loss=0.0635 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2425 | val_loss=0.0676 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2430 | val_loss=0.0450 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2435 | val_loss=0.0940 acc=0.982 f1=0.983 auc=0.998\n",
      "Epoch 2440 | val_loss=0.0957 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2445 | val_loss=0.1014 acc=0.982 f1=0.983 auc=0.999\n",
      "Epoch 2450 | val_loss=0.0654 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2455 | val_loss=0.0539 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2460 | val_loss=0.0728 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2465 | val_loss=0.0868 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2470 | val_loss=0.0543 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 2475 | val_loss=0.0321 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2480 | val_loss=0.0612 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2485 | val_loss=0.0547 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2490 | val_loss=0.0365 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2495 | val_loss=0.0380 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2500 | val_loss=0.0445 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2505 | val_loss=0.0420 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2510 | val_loss=0.0380 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2515 | val_loss=0.0313 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2520 | val_loss=0.0308 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 2525 | val_loss=0.0304 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2530 | val_loss=0.0335 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2535 | val_loss=0.0510 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2540 | val_loss=0.0398 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2545 | val_loss=0.0368 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2550 | val_loss=0.0459 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2555 | val_loss=0.0429 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2560 | val_loss=0.0434 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2565 | val_loss=0.0496 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2570 | val_loss=0.0926 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2575 | val_loss=0.0497 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2580 | val_loss=0.0658 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 2585 | val_loss=0.0520 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2590 | val_loss=0.0537 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2595 | val_loss=0.0416 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2600 | val_loss=0.0381 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2605 | val_loss=0.0467 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2610 | val_loss=0.0532 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2615 | val_loss=0.0698 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2620 | val_loss=0.0520 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2625 | val_loss=0.0663 acc=0.989 f1=0.989 auc=0.999\n",
      "Epoch 2630 | val_loss=0.0365 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2635 | val_loss=0.0478 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2640 | val_loss=0.0292 acc=0.993 f1=0.993 auc=1.000\n",
      "Epoch 2645 | val_loss=0.0405 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2650 | val_loss=0.0336 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2655 | val_loss=0.0473 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2660 | val_loss=0.0556 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2665 | val_loss=0.0503 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2670 | val_loss=0.0556 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2675 | val_loss=0.0575 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2680 | val_loss=0.0570 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2685 | val_loss=0.0367 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2690 | val_loss=0.0464 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2695 | val_loss=0.0322 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2700 | val_loss=0.0578 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2705 | val_loss=0.0413 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2710 | val_loss=0.0535 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2715 | val_loss=0.0864 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 2720 | val_loss=0.0396 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2725 | val_loss=0.0382 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2730 | val_loss=0.0420 acc=0.992 f1=0.992 auc=1.000\n",
      "Epoch 2735 | val_loss=0.0384 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2740 | val_loss=0.0618 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2745 | val_loss=0.0538 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2750 | val_loss=0.0510 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2755 | val_loss=0.0575 acc=0.990 f1=0.990 auc=0.999\n",
      "Epoch 2760 | val_loss=0.0708 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2765 | val_loss=0.0781 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2770 | val_loss=0.0571 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2775 | val_loss=0.0607 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2780 | val_loss=0.0515 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2785 | val_loss=0.0824 acc=0.987 f1=0.987 auc=0.998\n",
      "Epoch 2790 | val_loss=0.0617 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2795 | val_loss=0.0524 acc=0.991 f1=0.991 auc=0.999\n",
      "Epoch 2800 | val_loss=0.0664 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2805 | val_loss=0.0622 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2810 | val_loss=0.0640 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2815 | val_loss=0.0564 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2820 | val_loss=0.0770 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2825 | val_loss=0.0722 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2830 | val_loss=0.0843 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2835 | val_loss=0.0822 acc=0.980 f1=0.981 auc=0.999\n",
      "Epoch 2840 | val_loss=0.0749 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2845 | val_loss=0.0806 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2850 | val_loss=0.0681 acc=0.982 f1=0.983 auc=1.000\n",
      "Epoch 2855 | val_loss=0.0859 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2860 | val_loss=0.0721 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2865 | val_loss=0.0567 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2870 | val_loss=0.0893 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2875 | val_loss=0.0802 acc=0.983 f1=0.984 auc=1.000\n",
      "Epoch 2880 | val_loss=0.0883 acc=0.981 f1=0.982 auc=0.998\n",
      "Epoch 2885 | val_loss=0.0976 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2890 | val_loss=0.0879 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2895 | val_loss=0.0659 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2900 | val_loss=0.0552 acc=0.989 f1=0.989 auc=1.000\n",
      "Epoch 2905 | val_loss=0.1098 acc=0.977 f1=0.977 auc=0.999\n",
      "Epoch 2910 | val_loss=0.1193 acc=0.980 f1=0.981 auc=0.998\n",
      "Epoch 2915 | val_loss=0.0714 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2920 | val_loss=0.0768 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2925 | val_loss=0.0578 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 2930 | val_loss=0.0829 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2935 | val_loss=0.0855 acc=0.986 f1=0.986 auc=0.999\n",
      "Epoch 2940 | val_loss=0.0888 acc=0.987 f1=0.987 auc=1.000\n",
      "Epoch 2945 | val_loss=0.0874 acc=0.985 f1=0.985 auc=1.000\n",
      "Epoch 2950 | val_loss=0.0660 acc=0.988 f1=0.988 auc=1.000\n",
      "Epoch 2955 | val_loss=0.0606 acc=0.987 f1=0.987 auc=0.999\n",
      "Epoch 2960 | val_loss=0.0824 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2965 | val_loss=0.0636 acc=0.988 f1=0.988 auc=0.999\n",
      "Epoch 2970 | val_loss=0.0809 acc=0.983 f1=0.984 auc=0.999\n",
      "Epoch 2975 | val_loss=0.0643 acc=0.985 f1=0.985 auc=0.999\n",
      "Epoch 2980 | val_loss=0.0840 acc=0.986 f1=0.986 auc=1.000\n",
      "Epoch 2985 | val_loss=0.0537 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2990 | val_loss=0.0624 acc=0.990 f1=0.990 auc=1.000\n",
      "Epoch 2995 | val_loss=0.0470 acc=0.991 f1=0.991 auc=1.000\n",
      "Epoch 3000 | val_loss=0.0560 acc=0.987 f1=0.987 auc=0.999\n",
      "Best @ epoch 1755 val_acc 0.994\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# DataLoaders (batch sizes unchanged)\n",
    "Xtr = torch.from_numpy(X_train_bal).to(device)\n",
    "ytr = torch.from_numpy(y_train_bal.astype(np.float32)).to(device)\n",
    "Xte = torch.from_numpy(X_test).to(device)\n",
    "yte = torch.from_numpy(y_test.astype(np.float32)).to(device)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr,ytr), batch_size=256, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(Xte,yte), batch_size=512, shuffle=False)\n",
    "\n",
    "model = HybridBNN(in_dim=X_train.shape[1]).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # unweighted loss\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss, n = 0.0, 0\n",
    "    all_logits, all_y = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            total_loss += float(loss.item()) * xb.size(0)\n",
    "            n += xb.size(0)\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_y.append(yb.detach().cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    y_true = torch.cat(all_y).numpy()\n",
    "    probs = 1/(1+np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    return total_loss/max(1,n), acc, f1, auc\n",
    "\n",
    "best = {\"epoch\":-1, \"loss\":1e9, \"acc\":0.0, \"state\":None}\n",
    "EPOCHS = 3000\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "    val_loss, acc, f1, auc = evaluate()\n",
    "    if val_loss < best[\"loss\"]:\n",
    "        best.update({\"epoch\":epoch, \"loss\":val_loss, \"acc\":acc, \"state\":model.state_dict()})\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:02d} | val_loss={val_loss:.4f} acc={acc:.3f} f1={f1:.3f} auc={auc:.3f}\")\n",
    "\n",
    "model.load_state_dict(best[\"state\"])\n",
    "print(\"Best @ epoch\", best[\"epoch\"], \"val_acc\", round(best[\"acc\"],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Evaluation + Threshold Sweep (find a useful operating point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9736    0.9866       454\n",
      "           1     0.9742    1.0000    0.9870       454\n",
      "\n",
      "    accuracy                         0.9868       908\n",
      "   macro avg     0.9871    0.9868    0.9868       908\n",
      "weighted avg     0.9871    0.9868    0.9868       908\n",
      "\n",
      "ROC-AUC: 0.9994323584777504\n",
      "PR-AUC : 0.9991173931635794\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.9996129870414734 Best F1: 0.9989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9978    0.9989       454\n",
      "           1     0.9978    1.0000    0.9989       454\n",
      "\n",
      "    accuracy                         0.9989       908\n",
      "   macro avg     0.9989    0.9989    0.9989       908\n",
      "weighted avg     0.9989    0.9989    0.9989       908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.from_numpy(X_test).to(device)).cpu().numpy()\n",
    "probs = 1/(1+np.exp(-logits))\n",
    "preds = (probs>=0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, preds, digits=4))\n",
    "try:\n",
    "    print('ROC-AUC:', roc_auc_score(y_test, probs))\n",
    "    print('PR-AUC :', average_precision_score(y_test, probs))\n",
    "except Exception as e:\n",
    "    print('AUC error:', e)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plt.figure(); plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "plt.title('Confusion Matrix'); plt.colorbar();\n",
    "plt.xticks([0,1], ['No Asthma','Asthma']); plt.yticks([0,1], ['No Asthma','Asthma'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center')\n",
    "plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Threshold sweep (F1)\n",
    "prec, rec, thr = precision_recall_curve(y_test, probs)\n",
    "best_t, best_f1 = 0.5, -1.0\n",
    "for t in np.unique(np.clip(thr, 0, 1)):\n",
    "    pred = (probs >= t).astype(int)\n",
    "    f1 = f1_score(y_test, pred, zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_t = f1, float(t)\n",
    "print('Best threshold:', best_t, 'Best F1:', round(best_f1,4))\n",
    "print(classification_report(y_test, (probs>=best_t).astype(int), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Save Artifacts (unchanged path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to bnn_artifacts\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path('bnn_artifacts')  # unchanged\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "torch.save(model.state_dict(), out_dir/'bnn_state.pt')\n",
    "with open(out_dir/'scaler.json','w') as f:\n",
    "    json.dump({'mean': x_mean.tolist(), 'scale': x_scale.tolist(), 'predictors': predictors, 'target': target_col}, f)\n",
    "print('Saved to', out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Robust Prediction Helper (exact-forward; path auto-discovery inc. `/mnt/data/bnn_artifacts`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CANDIDATE_DIRS = ['bnn_artifacts','./bnn_artifacts','/mnt/data/bnn_artifacts', str(Path.cwd()/'bnn_artifacts')]\n",
    "_model_cache = {\"legacy\": None, \"z2p1\": None}\n",
    "\n",
    "def _find_art_dir():\n",
    "    for p in _CANDIDATE_DIRS:\n",
    "        d = Path(p)\n",
    "        if (d/'bnn_state.pt').exists() and (d/'scaler.json').exists():\n",
    "            return d\n",
    "    raise FileNotFoundError('Artifacts not found in: ' + ', '.join(_CANDIDATE_DIRS))\n",
    "\n",
    "def _infer_hidden_from_state(state, input_dim):\n",
    "    w2d = [(k,v) for k,v in state.items() if k.endswith('.weight') and hasattr(v,'ndim') and v.ndim==2]\n",
    "    hidden, first = [], False\n",
    "    for k,W in w2d:\n",
    "        out_f, in_f = W.shape\n",
    "        if not first:\n",
    "            if in_f != input_dim: continue\n",
    "            hidden.append(out_f); first = True; continue\n",
    "        if out_f == 1: break\n",
    "        hidden.append(out_f)\n",
    "    if not hidden: raise ValueError(\"Failed to infer hidden sizes from checkpoint.\")\n",
    "    return hidden\n",
    "\n",
    "def _build_bnn_from_state(state, input_dim, p_drop=0.1, binarize_mode=\"legacy\"):\n",
    "    hidden = _infer_hidden_from_state(state, input_dim)\n",
    "    m = HybridBNN(input_dim, hidden, p_drop=p_drop, binarize_mode=binarize_mode)\n",
    "    m._inferred_hidden = hidden\n",
    "    return m\n",
    "\n",
    "def _load_artifacts_for_infer(binarize_mode=\"legacy\"):\n",
    "    art = _find_art_dir()\n",
    "    meta = json.loads((art/'scaler.json').read_text())\n",
    "    preds  = meta['predictors']\n",
    "    x_mean = np.array(meta['mean'],  dtype=np.float32)\n",
    "    x_scale= np.array(meta['scale'], dtype=np.float32)\n",
    "    # honor metadata hint if present\n",
    "    bm = str(meta.get('binarize_mode', binarize_mode)).lower()\n",
    "    if bm in {\"legacy\",\"z2p1\"}: binarize_mode = bm\n",
    "    state = torch.load(art/'bnn_state.pt', map_location='cpu')\n",
    "    m = _build_bnn_from_state(state, input_dim=len(preds), p_drop=0.1, binarize_mode=binarize_mode)\n",
    "    m.load_state_dict(state, strict=True)\n",
    "    m.eval()\n",
    "    print(f\"✅ Loaded from {art} | hidden sizes: {m._inferred_hidden} | binarize={binarize_mode}\")\n",
    "    return m, preds, x_mean, x_scale\n",
    "\n",
    "def _get_model(binarize_mode=\"legacy\"):\n",
    "    if _model_cache[binarize_mode] is None:\n",
    "        _model_cache[binarize_mode] = _load_artifacts_for_infer(binarize_mode)\n",
    "    return _model_cache[binarize_mode]\n",
    "\n",
    "def reset_infer_cache():\n",
    "    for k in _model_cache: _model_cache[k] = None\n",
    "\n",
    "# ---------- preprocessing & prediction ----------\n",
    "def _prep_df(df_like, predictors, x_mean, x_scale, strict=False):\n",
    "    if isinstance(df_like, dict):\n",
    "        df = pd.DataFrame([df_like])\n",
    "    elif isinstance(df_like, pd.Series):\n",
    "        df = pd.DataFrame([df_like.to_dict()])\n",
    "    else:\n",
    "        df = df_like.copy()\n",
    "\n",
    "    # enforce exact predictor order; fill missing with training mean (warn)\n",
    "    missing = [c for c in predictors if c not in df.columns]\n",
    "    if missing:\n",
    "        if strict:\n",
    "            raise KeyError(f\"Missing predictors: {missing}\")\n",
    "        print(f\"[WARN] Filling {len(missing)} missing column(s) with training mean: {missing[:8]}{'...' if len(missing)>8 else ''}\")\n",
    "        for c, mu in zip(predictors, x_mean):\n",
    "            if c not in df.columns:\n",
    "                df[c] = float(mu)\n",
    "\n",
    "    X = df[predictors].astype(np.float32).values\n",
    "    Xn = (X - x_mean) / (x_scale + 1e-8)\n",
    "    return Xn\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_asthma(df_like, threshold: float = 0.5, binarize_mode=\"legacy\", strict=False, verbose=False):\n",
    "    m, preds, mu, sigma = _get_model(binarize_mode)\n",
    "    Xn = _prep_df(df_like, preds, mu, sigma, strict=strict)\n",
    "    lg = m(torch.from_numpy(Xn)).cpu().numpy()\n",
    "    pr = 1/(1+np.exp(-lg))\n",
    "    lb = (pr >= float(threshold)).astype(np.int32)\n",
    "    if verbose:\n",
    "        print(f\"[{binarize_mode}] logits: min={lg.min():.4f} max={lg.max():.4f} mean={lg.mean():.4f}\")\n",
    "        print(f\"[{binarize_mode}] probs : min={pr.min():.4f} max={pr.max():.4f} mean={pr.mean():.4f}\")\n",
    "    return pr, lb, lg\n",
    "\n",
    "def compare_binarize_modes(df_like, threshold=0.5, strict=False):\n",
    "    out = {}\n",
    "    for mode in [\"legacy\",\"z2p1\"]:\n",
    "        try:\n",
    "            out[mode] = predict_asthma(df_like, threshold, binarize_mode=mode, strict=strict, verbose=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[{mode}] ERROR:\", e); out[mode]=None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Parameter Size (KiB) — confirm ≤ 20KB total (fold BN for deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- size estimator ----------\n",
    "def param_size_kib(in_dim: int, hidden: list, out_dim=1, first_layer_float=True, bn_mode=\"fold\"):\n",
    "    assert bn_mode in {\"fold\",\"affine\",\"all\"}\n",
    "    dims = [in_dim] + list(hidden) + [out_dim]\n",
    "    weight_bytes = 0\n",
    "    for i in range(len(dims)-1):\n",
    "        fan_in, fan_out = dims[i], dims[i+1]\n",
    "        if i==0 and first_layer_float:\n",
    "            weight_bytes += fan_in*fan_out*4\n",
    "        else:\n",
    "            weight_bytes += ((fan_in*fan_out) + 7)//8\n",
    "    bias_bytes = 4  # output bias only in this architecture\n",
    "    if bn_mode==\"fold\":\n",
    "        bn_bytes = 0\n",
    "    else:\n",
    "        bn_channels = hidden[0] + sum(hidden[1:])\n",
    "        bn_bytes = (2 if bn_mode==\"affine\" else 4)*bn_channels*4\n",
    "    total = weight_bytes + bias_bytes + bn_bytes\n",
    "    return {\n",
    "        \"weight_kib\": round(weight_bytes/1024,4),\n",
    "        \"bias_kib\": round(bias_bytes/1024,4),\n",
    "        \"bn_kib\": round(bn_bytes/1024,4),\n",
    "        \"total_kib\": round(total/1024,4),\n",
    "    }\n",
    "\n",
    "def print_loaded_model_size(first_layer_float=True, bn_mode=\"fold\", binarize_mode=\"legacy\"):\n",
    "    m, preds, *_ = _get_model(binarize_mode)\n",
    "    sizes = param_size_kib(len(preds), m._inferred_hidden, 1, first_layer_float, bn_mode)\n",
    "    print(f\"Estimated deployment size (binarize={binarize_mode}, BN={bn_mode}):\")\n",
    "    for k,v in sizes.items(): print(f\"  {k}: {v} KiB\")\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Quick Usage Examples (unchanged paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch probs: [0.00e+00 1.55e-02 0.00e+00 0.00e+00 3.00e-04 0.00e+00 0.00e+00 1.00e-04]\n",
      "Batch labels: [0 0 0 0 0 0 0 0]\n",
      "Batch logits: [-12.9575  -4.1516 -12.9575 -12.1569  -8.1542 -12.9575 -12.1569  -9.7553]\n",
      "Asthma prob (obvious case): 0.9999929666519165 Label: 1\n",
      "Logit: 11.859176635742188\n"
     ]
    }
   ],
   "source": [
    "# Batch check\n",
    "probs_b, labels_b, logits_b = predict_asthma(pd.read_csv('asthma_disease_data.csv').head(8))\n",
    "print('Batch probs:', probs_b.round(4))\n",
    "print('Batch labels:', labels_b)\n",
    "print('Batch logits:', logits_b.round(4))\n",
    "\n",
    "# Strong positive (obvious asthma) example row\n",
    "row_asthma = {\n",
    "    'Age': 25,\n",
    "    'Gender': 1,\n",
    "    'BMI': 39.29764739,\n",
    "    'Smoking': 0,\n",
    "    'PhysicalActivity': 8.899044846,\n",
    "    'DietQuality': 0.325397968,\n",
    "    'SleepQuality': 5.524751815,\n",
    "    'PollutionExposure': 7.854229872,\n",
    "    'PollenExposure': 0.498309572,\n",
    "    'DustExposure': 5.133637227,\n",
    "    'PetAllergy': 0,\n",
    "    'FamilyHistoryAsthma': 1,\n",
    "    'HistoryOfAllergies': 0,\n",
    "    'Eczema': 0,\n",
    "    'HayFever': 1,\n",
    "    'GastroesophagealReflux': 0,\n",
    "    'Wheezing': 1,\n",
    "    'ShortnessOfBreath': 0,\n",
    "    'ChestTightness': 0,\n",
    "    'Coughing': 0,\n",
    "    'NighttimeSymptoms': 0,\n",
    "    'ExerciseInduced': 1\n",
    "}\n",
    "\n",
    "p_asthma, l_asthma, logits = predict_asthma(row_asthma, threshold=0.5)\n",
    "print('Asthma prob (obvious case):', float(p_asthma[0]), 'Label:', int(l_asthma[0]))\n",
    "print('Logit:', float(logits[0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
