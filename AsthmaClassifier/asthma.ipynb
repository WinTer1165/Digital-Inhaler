{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESP32-friendly Binary Neural Net (BNN): **Train → Test → Export** (No Synthetic Fallback)\n",
    "\n",
    "This notebook includes:\n",
    "- **Training** (`train`) — GPU-aware, multilabel (3 outputs)\n",
    "- **Testing** (`test_model`, `test_accuracy`) — macro/per-class F1, subset & hamming accuracy\n",
    "- **Export** (`export_to_header`) — bit-packed `bnn_export.h` for ESP32 (XNOR+POPCOUNT inference)\n",
    "\n",
    "**Important:** You must set `CSV_PATH` to a valid CSV file. The CSV must have **features in all columns except the last 3**, which are binary labels (0/1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "safe_torch_load ready\n"
     ]
    }
   ],
   "source": [
    "# Environment & compatibility\n",
    "import os, math, csv, random, json\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# PyTorch 2.6+ safe-load allowlist for older checkpoints that include NumPy objects\n",
    "try:\n",
    "    from torch.serialization import add_safe_globals\n",
    "    import numpy as _np\n",
    "    add_safe_globals([_np.core.multiarray._reconstruct])\n",
    "    def safe_torch_load(path, map_location='cpu'):\n",
    "        return torch.load(path, map_location=map_location, weights_only=False)\n",
    "except Exception:\n",
    "    def safe_torch_load(path, map_location='cpu'):\n",
    "        return torch.load(path, map_location=map_location)\n",
    "print('safe_torch_load ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader (CSV)\n",
    "- CSV with header; **last 3 columns** are labels (0/1)\n",
    "- Features standardized (mean/std exported for ESP32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularMultiLabel(Dataset):\n",
    "    def __init__(self, csv_path: str):\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f'CSV not found: {csv_path}')\n",
    "        rows = []\n",
    "        with open(csv_path, 'r', newline='') as f:\n",
    "            r = csv.reader(f)\n",
    "            header = next(r)\n",
    "            for row in r:\n",
    "                if not row: continue\n",
    "                rows.append([float(x) for x in row])\n",
    "        arr = np.array(rows, dtype=np.float32)\n",
    "        X = arr[:, :-3]\n",
    "        y = arr[:, -3:]\n",
    "        self.mean = X.mean(axis=0, keepdims=True)\n",
    "        self.std = X.std(axis=0, keepdims=True) + 1e-6\n",
    "        self.X = (X - self.mean) / self.std\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary layers & model (hidden layers binarized with STE)\n",
    "- Hidden weights & activations are binary; final layer is real-valued\n",
    "- Suggests hidden sizes to fit ~**20 KB** bit-packed parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(t: torch.Tensor):\n",
    "    return t.sign().clamp(min=-1., max=1.)\n",
    "\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_f, out_f, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_f, in_f))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_f)) if bias else None\n",
    "        nn.init.kaiming_normal_(self.weight)\n",
    "    def forward(self, x):\n",
    "        w = binarize(self.weight) + (self.weight - self.weight.detach())\n",
    "        return F.linear(x, w, self.bias)\n",
    "\n",
    "class BinaryAct(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return binarize(x) + (x - x.detach())\n",
    "\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self, in_f: int, hidden_sizes: Tuple[int, ...], out_f: int = 3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_f\n",
    "        for h in hidden_sizes:\n",
    "            layers += [BinaryLinear(last, h), nn.BatchNorm1d(h), BinaryAct()]\n",
    "            last = h\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "        self.fc_out = nn.Linear(last, out_f)\n",
    "    def forward(self, x):\n",
    "        h = self.hidden(x)\n",
    "        return self.fc_out(h)\n",
    "\n",
    "def packed_bytes(bits:int):\n",
    "    return (bits + 7) // 8\n",
    "\n",
    "def suggest_hidden_sizes(in_f: int, out_f: int = 3, budget_bytes: int = 20*1024):\n",
    "    candidates = [256, 128, 64, 32]\n",
    "    best = None\n",
    "    for a in candidates:\n",
    "        for b in candidates:\n",
    "            for c in candidates:\n",
    "                hs = [a,b,c]\n",
    "                bits = in_f*a + a*b + b*c\n",
    "                bits += 8*(c*out_f)\n",
    "                if packed_bytes(bits) <= budget_bytes:\n",
    "                    if best is None or sum(hs) > sum(best):\n",
    "                        best = hs\n",
    "    if best is None:\n",
    "        for a in candidates[::-1]:\n",
    "            bits = in_f*a + 8*(a*out_f)\n",
    "            if packed_bytes(bits) <= budget_bytes:\n",
    "                return [a]\n",
    "        return [32]\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test functions\n",
    "- `train(...)` saves `bnn_best.pt` with weights + metadata (in_f, hidden, mean, std)\n",
    "- `test_model(...)` reloads and reports macro/per-class F1\n",
    "- `test_accuracy(...)` adds subset & hamming accuracy, per-class accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(csv_path: str, epochs=20, batch_size=2048, lr=5e-4):\n",
    "    ds = TabularMultiLabel(csv_path)\n",
    "    n = len(ds)\n",
    "    in_f = ds.X.shape[1]\n",
    "    out_f = 3\n",
    "    if n > 320000:\n",
    "        train_n = 300000\n",
    "        test_n = n - train_n\n",
    "        train_ds, test_ds = random_split(ds, [train_n, test_n], generator=torch.Generator().manual_seed(SEED))\n",
    "    else:\n",
    "        train_n = int(0.8 * n)\n",
    "        test_n = n - train_n\n",
    "        train_ds, test_ds = random_split(ds, [train_n, test_n], generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "    hs = suggest_hidden_sizes(in_f, out_f, budget_bytes=20*1024)\n",
    "    print(f\"Input={in_f}, Hidden={hs}, Output=3 (budget ~20KB)\")\n",
    "    model = BNN(in_f, tuple(hs), out_f).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=(device.type=='cuda'))\n",
    "    test_loader = DataLoader(test_ds, batch_size=4096, shuffle=False, num_workers=0)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); total=0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            opt.zero_grad(); logits = model(xb)\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, yb)\n",
    "            loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step(); total += loss.item()*xb.size(0)\n",
    "        sched.step()\n",
    "        # Eval\n",
    "        model.eval(); tp=np.zeros(3); fp=np.zeros(3); fn=np.zeros(3)\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                probs = torch.sigmoid(model(xb)); preds = (probs>=0.5).float()\n",
    "                y=yb.cpu().numpy(); p=preds.cpu().numpy()\n",
    "                tp += (((p==1)&(y==1)).sum(axis=0)); fp += (((p==1)&(y==0)).sum(axis=0)); fn += (((p==0)&(y==1)).sum(axis=0))\n",
    "        f1=[]\n",
    "        for i in range(3):\n",
    "            precision = tp[i]/(tp[i]+fp[i]+1e-9); recall = tp[i]/(tp[i]+fn[i]+1e-9)\n",
    "            f1.append(2*precision*recall/(precision+recall+1e-9))\n",
    "        macro=float(np.mean(f1))\n",
    "        print(f\"Epoch {ep:02d} | loss={total/max(1,train_n):.4f} | macroF1={macro:.4f} | per-class F1={f1}\")\n",
    "        if macro>best_f1:\n",
    "            best_f1=macro\n",
    "            torch.save({'state_dict':model.state_dict(),'in_f':in_f,'hidden':hs,'mean':ds.mean,'std':ds.std}, 'bnn_best.pt')\n",
    "            print(f\"Saved bnn_best.pt (macroF1={best_f1:.4f})\")\n",
    "    print('Best macroF1=', best_f1)\n",
    "\n",
    "def test_model(ckpt_path='bnn_best.pt', csv_path=None, threshold=0.5):\n",
    "    if csv_path is None:\n",
    "        raise ValueError('csv_path is required')\n",
    "    ckpt = safe_torch_load(ckpt_path, map_location=device)\n",
    "    state=ckpt['state_dict']; in_f=ckpt['in_f']; hs=ckpt['hidden']\n",
    "    model = BNN(in_f, tuple(hs), out_f=3).to(device); model.load_state_dict(state); model.eval()\n",
    "    ds = TabularMultiLabel(csv_path)\n",
    "    loader = DataLoader(ds, batch_size=4096, shuffle=False)\n",
    "    tp=np.zeros(3); fp=np.zeros(3); fn=np.zeros(3)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb=xb.to(device); yb=yb.to(device)\n",
    "            probs=torch.sigmoid(model(xb)); preds=(probs>=threshold).float()\n",
    "            y=yb.cpu().numpy(); p=preds.cpu().numpy()\n",
    "            tp += (((p==1)&(y==1)).sum(axis=0)); fp += (((p==1)&(y==0)).sum(axis=0)); fn += (((p==0)&(y==1)).sum(axis=0))\n",
    "    f1=[]\n",
    "    for i in range(3):\n",
    "        precision = tp[i]/(tp[i]+fp[i]+1e-9); recall = tp[i]/(tp[i]+fn[i]+1e-9)\n",
    "        f1.append(2*precision*recall/(precision+recall+1e-9))\n",
    "    macro=float(np.mean(f1))\n",
    "    print(f\"Test macroF1={macro:.4f} | per-class F1={f1}\")\n",
    "    return macro, f1\n",
    "\n",
    "def test_accuracy(ckpt_path='bnn_best.pt', csv_path=None, threshold=0.5):\n",
    "    if csv_path is None:\n",
    "        raise ValueError('csv_path is required')\n",
    "    ckpt = safe_torch_load(ckpt_path, map_location=device)\n",
    "    state, in_f, hs = ckpt['state_dict'], ckpt['in_f'], ckpt['hidden']\n",
    "    model = BNN(in_f, tuple(hs), out_f=3).to(device)\n",
    "    model.load_state_dict(state); model.eval()\n",
    "    ds = TabularMultiLabel(csv_path)\n",
    "    loader = DataLoader(ds, batch_size=4096, shuffle=False)\n",
    "    import numpy as np\n",
    "    tp = np.zeros(3); fp = np.zeros(3); fn = np.zeros(3); tn = np.zeros(3)\n",
    "    total_samples = 0\n",
    "    subset_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            probs = torch.sigmoid(model(xb))\n",
    "            preds = (probs >= threshold).float()\n",
    "            y = yb.cpu().numpy()\n",
    "            p = preds.cpu().numpy()\n",
    "            tp += (((p==1)&(y==1)).sum(axis=0))\n",
    "            fp += (((p==1)&(y==0)).sum(axis=0))\n",
    "            fn += (((p==0)&(y==1)).sum(axis=0))\n",
    "            tn += (((p==0)&(y==0)).sum(axis=0))\n",
    "            subset_correct += (p == y).all(axis=1).sum()\n",
    "            total_samples += y.shape[0]\n",
    "    f1 = []\n",
    "    for i in range(3):\n",
    "        precision = tp[i] / (tp[i] + fp[i] + 1e-9)\n",
    "        recall    = tp[i] / (tp[i] + fn[i] + 1e-9)\n",
    "        f1.append(2*precision*recall/(precision+recall+1e-9))\n",
    "    macro_f1 = float(np.mean(f1))\n",
    "    per_class_acc   = (tp + tn) / (tp + tn + fp + fn + 1e-9)\n",
    "    hamming_acc     = float((tp + tn).sum() / (total_samples * 3.0))\n",
    "    subset_accuracy = float(subset_correct / max(1, total_samples))\n",
    "    print(f\"Test @ thr={threshold} | subset_acc={subset_accuracy:.4f} | hamming_acc={hamming_acc:.4f} | per-class_acc={per_class_acc.tolist()} | macroF1={macro_f1:.4f}\")\n",
    "    return {\n",
    "        'subset_accuracy': subset_accuracy,\n",
    "        'hamming_accuracy': hamming_acc,\n",
    "        'per_class_accuracy': per_class_acc.tolist(),\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': [float(x) for x in f1]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ESP32 header (`bnn_export.h`) — bit-packed binary weights\n",
    "Packs only **2-D Linear weights** (skips 1-D BatchNorm parameters). Includes a safety check on the number of hidden layers found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_bits(W: np.ndarray):\n",
    "    signs = (np.sign(W) > 0).astype(np.uint8)\n",
    "    if signs.ndim != 2:\n",
    "        raise ValueError(f'pack_bits expects 2D, got shape {signs.shape}')\n",
    "    out_f, in_f = signs.shape\n",
    "    packed = []\n",
    "    for o in range(out_f):\n",
    "        row = signs[o]\n",
    "        byte = 0\n",
    "        for i in range(in_f):\n",
    "            bit = int(row[i])\n",
    "            byte |= (bit << (i % 8))\n",
    "            if (i % 8) == 7:\n",
    "                packed.append(byte); byte = 0\n",
    "        if (in_f % 8) != 0:\n",
    "            packed.append(byte)\n",
    "    return np.frombuffer(bytearray(packed), dtype=np.uint8)\n",
    "\n",
    "def export_to_header(ckpt_path='bnn_best.pt', header_path='bnn_export.h'):\n",
    "    ckpt = safe_torch_load(ckpt_path, map_location='cpu')\n",
    "    state = ckpt['state_dict']\n",
    "    in_f  = ckpt['in_f']\n",
    "    hs    = ckpt['hidden']\n",
    "    mean  = ckpt['mean'].astype(np.float32)\n",
    "    std   = ckpt['std'].astype(np.float32)\n",
    "    # Collect ONLY 2D weights (Linear layers); skip BatchNorm (1D)\n",
    "    bin_ws = []\n",
    "    for k, v in state.items():\n",
    "        if k.startswith('hidden.') and k.endswith('.weight') and v.dim() == 2:\n",
    "            bin_ws.append(v.numpy())\n",
    "    if len(bin_ws) != len(hs):\n",
    "        keys = [k for k,v in state.items() if k.startswith('hidden.') and k.endswith('.weight')]\n",
    "        raise RuntimeError(f'Expected {len(hs)} binary layers, found {len(bin_ws)}. Keys: {keys}')\n",
    "    Wout = state['fc_out.weight'].numpy().astype(np.float32)\n",
    "    Bout = state['fc_out.bias'].numpy().astype(np.float32)\n",
    "    packed_ws = [pack_bits(W) for W in bin_ws]\n",
    "    with open(header_path, 'w') as f:\n",
    "        f.write('// Auto-generated: ESP32 BNN export\\n#pragma once\\n\\n')\n",
    "        f.write(f'#define IN_F {in_f}\\n#define OUT_F 3\\n')\n",
    "        if len(hs)>0: f.write(f'#define H1 {hs[0]}\\n')\n",
    "        if len(hs)>1: f.write(f'#define H2 {hs[1]}\\n')\n",
    "        if len(hs)>2: f.write(f'#define H3 {hs[2]}\\n')\n",
    "        f.write('const float FEAT_MEAN[IN_F] = {'+','.join(str(float(x)) for x in mean.flatten())+'};\\n')\n",
    "        f.write('const float FEAT_STD[IN_F]  = {'+','.join(str(float(x)) for x in std.flatten())+'};\\n\\n')\n",
    "        for li, pw in enumerate(packed_ws):\n",
    "            f.write('const uint8_t Wb_'+str(li)+'['+str(len(pw))+'] = {'+','.join(str(int(x)) for x in pw)+'};\\n')\n",
    "            inf = in_f if li==0 else hs[li-1]\n",
    "            outf = hs[li]\n",
    "            f.write('const int Wb_'+str(li)+'_IN = '+str(inf)+';\\n')\n",
    "            f.write('const int Wb_'+str(li)+'_OUT = '+str(outf)+';\\n\\n')\n",
    "        f.write('const float Wout['+str(Wout.size)+'] = {'+','.join(str(float(x)) for x in Wout.flatten())+'};\\n')\n",
    "        f.write('const float Bout[3] = {'+','.join(str(float(x)) for x in Bout.flatten())+'};\\n')\n",
    "    print('Wrote', header_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & End-to-End Run (No Fallback)\n",
    "**Set `CSV_PATH`** to your dataset path. The cell asserts that the file exists and then runs **train → export → test**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset at processed-data.csv\n",
      "Input=16, Hidden=[256, 256, 256], Output=3 (budget ~20KB)\n",
      "Epoch 01 | loss=0.6696 | macroF1=0.1626 | per-class F1=[0.1490328902124288, 0.16828132040234176, 0.1705541153083121]\n",
      "Saved bnn_best.pt (macroF1=0.1626)\n",
      "Epoch 02 | loss=0.5938 | macroF1=0.0207 | per-class F1=[0.014700432309280979, 0.024028012472431675, 0.023403623303903227]\n",
      "Epoch 03 | loss=0.5774 | macroF1=0.0045 | per-class F1=[0.004156694778129246, 0.006476119281688316, 0.002867830412657532]\n",
      "Epoch 04 | loss=0.5739 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 05 | loss=0.5712 | macroF1=0.0003 | per-class F1=[0.00025414575017760753, 0.0, 0.0006262525026330138]\n",
      "Epoch 06 | loss=0.5696 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 07 | loss=0.5688 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 08 | loss=0.5680 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 09 | loss=0.5674 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 10 | loss=0.5671 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 11 | loss=0.5667 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 12 | loss=0.5664 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 13 | loss=0.5662 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 14 | loss=0.5659 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 15 | loss=0.5657 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 16 | loss=0.5658 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 17 | loss=0.5656 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 18 | loss=0.5654 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 19 | loss=0.5654 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Epoch 20 | loss=0.5654 | macroF1=0.0000 | per-class F1=[0.0, 0.0, 0.0]\n",
      "Best macroF1= 0.16262277530769423\n",
      "Wrote bnn_export.h\n",
      "Test macroF1=0.1645 | per-class F1=[0.14930924506463453, 0.17196819040357963, 0.1722939419514807]\n",
      "Test @ thr=0.5 | subset_acc=0.2264 | hamming_acc=0.6885 | per-class_acc=[0.6967803030303008, 0.6844696969696948, 0.6842803030303009] | macroF1=0.1645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subset_accuracy': 0.22642045454545454,\n",
       " 'hamming_accuracy': 0.688510101010101,\n",
       " 'per_class_accuracy': [0.6967803030303008,\n",
       "  0.6844696969696948,\n",
       "  0.6842803030303009],\n",
       " 'macro_f1': 0.16452379247323165,\n",
       " 'per_class_f1': [0.14930924506463453,\n",
       "  0.17196819040357963,\n",
       "  0.1722939419514807]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV_PATH = 'processed-data.csv'  # e.g., '/content/your_dataset.csv' (labels must be last 3 columns)\n",
    "assert CSV_PATH and os.path.exists(CSV_PATH), 'Please set CSV_PATH to your dataset (last 3 cols = labels).'\n",
    "print('Using dataset at', CSV_PATH)\n",
    "\n",
    "# === Run end-to-end ===\n",
    "train(CSV_PATH, epochs=20, batch_size=2048, lr=5e-4)\n",
    "export_to_header('bnn_best.pt', 'bnn_export.h')\n",
    "test_model('bnn_best.pt', CSV_PATH, threshold=0.5)\n",
    "test_accuracy('bnn_best.pt', CSV_PATH, threshold=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
